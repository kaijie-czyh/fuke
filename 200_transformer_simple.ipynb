{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 5 8 64\n",
      "1 5 8 64\n",
      "1 5 8 64\n",
      "1 5 8 64\n",
      "1 5 8 64\n",
      "1 5 8 64\n",
      "[[[-0.83330585 -0.35465986 -0.22141275 ... -1.25796649  0.97312025\n",
      "   -0.0480882 ]\n",
      "  [-0.83331168 -0.35466639 -0.22141783 ... -1.25796446  0.97310941\n",
      "   -0.04809868]\n",
      "  [-0.83330729 -0.35466209 -0.221414   ... -1.25796637  0.97311733\n",
      "   -0.0480919 ]\n",
      "  [-0.82366153 -0.34678935 -0.18550624 ... -1.27613497  1.03832769\n",
      "   -0.05364538]\n",
      "  [-0.8333002  -0.35465745 -0.22141123 ... -1.25796768  0.97312706\n",
      "   -0.04808093]]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 自定义softmax函数\n",
    "def softmax(x, axis=-1):\n",
    "    e_x = np.exp(x - np.max(x, axis=axis, keepdims=True))  # 防止溢出\n",
    "    return e_x / np.sum(e_x, axis=axis, keepdims=True)\n",
    "\n",
    "# 1. 生成输入的嵌入（这里只用简单的数字表示词）\n",
    "def get_embeddings(vocab_size, d_model):\n",
    "    return np.random.randn(vocab_size, d_model)\n",
    "\n",
    "# 2. 生成位置编码 (Positional Encoding)\n",
    "def get_positional_encoding(seq_len, d_model):\n",
    "    position = np.arange(seq_len).reshape(-1, 1)\n",
    "    div_term = np.exp(np.arange(0, d_model, 2) * -(np.log(10000.0) / d_model))\n",
    "    pe = np.zeros((seq_len, d_model))\n",
    "    pe[:, 0::2] = np.sin(position * div_term)\n",
    "    pe[:, 1::2] = np.cos(position * div_term)\n",
    "    return pe\n",
    "\n",
    "# 3. 自注意力机制 (Scaled Dot-Product Attention)\n",
    "def attention(query, key, value):\n",
    "    d_k = query.shape[-1]\n",
    "    scores = np.matmul(query, key.transpose(0, 2, 1)) / np.sqrt(d_k)\n",
    "    weights = softmax(scores, axis=-1)\n",
    "    return np.matmul(weights, value)\n",
    "\n",
    "# 4. 前馈神经网络 (Feed Forward Network)\n",
    "def feed_forward(x, d_ff):\n",
    "    w1 = np.random.randn(x.shape[-1], d_ff)\n",
    "    b1 = np.random.randn(d_ff)\n",
    "    w2 = np.random.randn(d_ff, x.shape[-1])\n",
    "    b2 = np.random.randn(x.shape[-1])\n",
    "    \n",
    "    # relu + 单核卷积(np.dot(x, w1) + b1)\n",
    "    x = np.maximum(0, np.dot(x, w1) + b1)\n",
    "    # 单核卷积\n",
    "    x = np.dot(x, w2) + b2\n",
    "    return x\n",
    "\n",
    "# 5. 多头自注意力机制 (Multi-Head Attention)\n",
    "def multi_head_attention(x, W_q, W_k, W_v, num_heads):\n",
    "    # 获取每个头的维度\n",
    "    d_model = x.shape[-1]\n",
    "    d_k = d_model // num_heads  # 每个头的维度\n",
    "    \n",
    "    # 对每个注意力头，分别计算 Q, K, V\n",
    "    # print(len(x), len(x[0]), len(x[0][0]))\n",
    "    # print(x)\n",
    "    \n",
    "    # print(len(np.dot(x, W_q)), len(np.dot(x, W_q)[0]), len(np.dot(x, W_q)[0][0]))\n",
    "    queries = np.dot(x, W_q).reshape(x.shape[0], x.shape[1], num_heads, d_k)\n",
    "    keys = np.dot(x, W_k).reshape(x.shape[0], x.shape[1], num_heads, d_k)\n",
    "    values = np.dot(x, W_v).reshape(x.shape[0], x.shape[1], num_heads, d_k)\n",
    "\n",
    "    print(x.shape[0], x.shape[1], num_heads, d_k)\n",
    "    # print(queries)\n",
    "    \n",
    "    # 计算每个头的注意力输出\n",
    "    heads = []\n",
    "    for i in range(num_heads):\n",
    "        head_output = attention(queries[:, :, i, :], keys[:, :, i, :], values[:, :, i, :])\n",
    "        heads.append(head_output)\n",
    "    \n",
    "    # 将每个头的输出拼接起来\n",
    "    concat_heads = np.concatenate(heads, axis=-1)\n",
    "    \n",
    "    # 通过 W_O 线性变换\n",
    "    W_O = np.random.randn(d_model, d_model)\n",
    "    return np.dot(concat_heads, W_O)\n",
    "\n",
    "# 6. 构建 Encoder 层\n",
    "def encoder_layer(x, W_q, W_k, W_v, d_model, num_heads, d_ff):\n",
    "    # 多头自注意力机制\n",
    "    attn_output = multi_head_attention(x, W_q, W_k, W_v, num_heads)\n",
    "    \n",
    "    # 残差连接\n",
    "    x = x + attn_output\n",
    "    # 层归一化 layer norm\n",
    "    x = (x - np.mean(x, axis=-1, keepdims=True)) / np.std(x, axis=-1, keepdims=True)\n",
    "    \n",
    "    ff_output = feed_forward(x, d_ff)\n",
    "    \n",
    "    # 残差连接\n",
    "    x = x + ff_output\n",
    "    # 层归一化\n",
    "    x = (x - np.mean(x, axis=-1, keepdims=True)) / np.std(x, axis=-1, keepdims=True)\n",
    "    \n",
    "    return x\n",
    "\n",
    "# 7. Transformer Encoder\n",
    "def transformer_encoder(x, W_q, W_k, W_v, d_model, num_heads, d_ff, num_layers):\n",
    "    seq_len = x.shape[1]\n",
    "    pos_encoding = get_positional_encoding(seq_len, d_model)\n",
    "    \n",
    "    x = x + pos_encoding\n",
    "    \n",
    "    for _ in range(num_layers):\n",
    "        x = encoder_layer(x, W_q, W_k, W_v, d_model, num_heads, d_ff)\n",
    "    \n",
    "    return x\n",
    "\n",
    "# 8. 简单解码器\n",
    "def simple_decoder(encoder_output, vocab_size, d_model):\n",
    "    decoder_output = np.dot(encoder_output, np.random.randn(d_model, vocab_size))  # 假设有一个线性层\n",
    "    return softmax(decoder_output, axis=-1)\n",
    "\n",
    "# 模拟中文翻译英文\n",
    "\n",
    "# 假设词汇表大小为 10，嵌入维度为 512，num_heads=8，d_ff=2048，num_layers=6\n",
    "vocab_size = 10  # 简化，假设词汇表大小为 10\n",
    "d_model = 512     # 嵌入维度 d_k = d_model // num_heads = 512 // 8 = 64\n",
    "num_heads = 8    # 自注意力头数\n",
    "d_ff = 2048      # 前馈神经网络的维度\n",
    "num_layers = 6   # Transformer 编码层数\n",
    "\n",
    "# 中文句子：我喜欢学习人工智能\n",
    "# 对应的英文翻译索引序列：['i', 'like', 'study', 'artificial', 'intelligence']\n",
    "# 目标输出为 'i like study artificial intelligence'\n",
    "target_seq = np.array([[5, 6, 7, 8, 9]])  # 模拟英文句子的词汇索引\n",
    "\n",
    "# 对应的索引序列：[0, 1, 2, 3, 4]\n",
    "x = np.array([[0, 1, 2, 3, 4]])  # 模拟中文句子的词汇索引\n",
    "\n",
    "# 获取嵌入表示\n",
    "embeddings = get_embeddings(vocab_size, d_model)\n",
    "x_embedded = embeddings[x]\n",
    "\n",
    "# 初始化线性变换矩阵 W_q, W_k, W_v\n",
    "W_q = np.random.randn(d_model, d_model)\n",
    "W_k = np.random.randn(d_model, d_model)\n",
    "W_v = np.random.randn(d_model, d_model)\n",
    "\n",
    "# print(len(W_q), len(W_q[0]))\n",
    "\n",
    "# 运行 Transformer 编码器\n",
    "encoder_output = transformer_encoder(x_embedded, W_q, W_k, W_v, d_model, num_heads, d_ff, num_layers)\n",
    "\n",
    "# 打印编码器输出\n",
    "print(encoder_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 5, 512)\n",
      "1 5 8 64\n",
      "1 5 8 64\n",
      "1 5 8 64\n",
      "1 5 8 64\n",
      "1 5 8 64\n",
      "1 5 8 64\n",
      "[[0. 1. 1. 1. 1.]\n",
      " [0. 0. 1. 1. 1.]\n",
      " [0. 0. 0. 1. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[[3.59618322e-13 1.81701812e-38 1.48694452e-22 5.58737314e-23\n",
      "   2.87964283e-20 7.33349586e-24 1.00000000e+00 3.40895063e-23\n",
      "   2.15758869e-11 1.59383056e-34]\n",
      "  [3.59618322e-13 1.81701812e-38 1.48694451e-22 5.58737315e-23\n",
      "   2.87964283e-20 7.33349586e-24 1.00000000e+00 3.40895064e-23\n",
      "   2.15758868e-11 1.59383056e-34]\n",
      "  [3.59618322e-13 1.81701812e-38 1.48694451e-22 5.58737315e-23\n",
      "   2.87964283e-20 7.33349586e-24 1.00000000e+00 3.40895064e-23\n",
      "   2.15758868e-11 1.59383056e-34]\n",
      "  [3.59618322e-13 1.81701812e-38 1.48694451e-22 5.58737315e-23\n",
      "   2.87964283e-20 7.33349586e-24 1.00000000e+00 3.40895064e-23\n",
      "   2.15758868e-11 1.59383056e-34]\n",
      "  [3.59618322e-13 1.81701812e-38 1.48694451e-22 5.58737315e-23\n",
      "   2.87964283e-20 7.33349586e-24 1.00000000e+00 3.40895064e-23\n",
      "   2.15758868e-11 1.59383056e-34]]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[6, 6, 6, 6, 6]], dtype=int64)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 8. 自注意力机制（解码器的部分，支持多头注意力）\n",
    "def masked_multi_head_attention(x, W_q, W_k, W_v, num_heads, mask=None):\n",
    "    # 获取每个头的维度\n",
    "    d_model = x.shape[-1]\n",
    "    d_k = d_model // num_heads  # 每个头的维度\n",
    "\n",
    "    # 对每个注意力头，分别计算 Q, K, V\n",
    "    queries = np.dot(x, W_q).reshape(x.shape[0], x.shape[1], num_heads, d_k)\n",
    "    keys = np.dot(x, W_k).reshape(x.shape[0], x.shape[1], num_heads, d_k)\n",
    "    values = np.dot(x, W_v).reshape(x.shape[0], x.shape[1], num_heads, d_k)\n",
    "    \n",
    "    # 计算每个头的注意力输出\n",
    "    heads = []\n",
    "    for i in range(num_heads):\n",
    "        scores = np.matmul(queries[:, :, i, :], keys[:, :, i, :].transpose(0, 2, 1)) / np.sqrt(d_k)\n",
    "        \n",
    "        # 应用 Mask，将未来时刻的权重置为负无穷\n",
    "        if mask is not None:\n",
    "            scores += (mask * -1e9)\n",
    "        \n",
    "        weights = softmax(scores, axis=-1)\n",
    "        heads.append(np.matmul(weights, values[:, :, i, :]))\n",
    "    \n",
    "    # 将每个头的输出拼接起来\n",
    "    concat_heads = np.concatenate(heads, axis=-1)\n",
    "    \n",
    "    # 通过 W_O 线性变换\n",
    "    W_O = np.random.randn(d_model, d_model)\n",
    "    return np.dot(concat_heads, W_O)\n",
    "\n",
    "# 9. 解码器层\n",
    "def decoder_layer(x, encoder_output, W_q, W_k, W_v, W_cross_q, W_cross_k, W_cross_v, d_model, num_heads, d_ff, look_ahead_mask=None):\n",
    "    # 自注意力机制\n",
    "    attn_output = masked_multi_head_attention(x, W_q, W_k, W_v, num_heads, look_ahead_mask)\n",
    "    x = x + attn_output  # 残差连接\n",
    "    x = (x - np.mean(x, axis=-1, keepdims=True)) / np.std(x, axis=-1, keepdims=True)  # 层归一化\n",
    "\n",
    "    # 交叉注意力机制：使用 encoder_output 作为键和值\n",
    "    cross_attn_output = masked_multi_head_attention(encoder_output, W_cross_q, W_cross_k, W_cross_v, num_heads)\n",
    "    x = x + cross_attn_output  # 残差连接\n",
    "    x = (x - np.mean(x, axis=-1, keepdims=True)) / np.std(x, axis=-1, keepdims=True)  # 层归一化\n",
    "\n",
    "    # 前馈神经网络\n",
    "    ff_output = feed_forward(x, d_ff)\n",
    "    x = x + ff_output  # 残差连接\n",
    "    x = (x - np.mean(x, axis=-1, keepdims=True)) / np.std(x, axis=-1, keepdims=True)  # 层归一化\n",
    "\n",
    "    return x\n",
    "\n",
    "# 10. Transformer 解码器\n",
    "def transformer_decoder(target, encoder_output, W_q, W_k, W_v, W_cross_q, W_cross_k, W_cross_v, d_model, num_heads, d_ff, num_layers):\n",
    "    seq_len = target.shape[1]\n",
    "    # print(target.shape)\n",
    "    pos_encoding = get_positional_encoding(seq_len, d_model)\n",
    "    \n",
    "    # 加入位置编码\n",
    "    target = target + pos_encoding\n",
    "    \n",
    "    # 构造 look-ahead mask (用于屏蔽未来的时间步)\n",
    "    look_ahead_mask = np.triu(np.ones((seq_len, seq_len)), k=1)\n",
    "\n",
    "    print(look_ahead_mask)\n",
    "    \n",
    "    for _ in range(num_layers):\n",
    "        target = decoder_layer(target, encoder_output, W_q, W_k, W_v, W_cross_q, W_cross_k, W_cross_v, d_model, num_heads, d_ff, look_ahead_mask)\n",
    "    \n",
    "    return target\n",
    "\n",
    "# 11. 完整的 Transformer 模型\n",
    "def transformer(x, target, vocab_size, d_model, num_heads, d_ff, num_layers):\n",
    "    # 获取嵌入表示\n",
    "    embeddings = get_embeddings(vocab_size, d_model)\n",
    "    x_embedded = embeddings[x]\n",
    "    target_embedded = embeddings[target]\n",
    "    print(target_embedded.shape)\n",
    "\n",
    "    # 初始化线性变换矩阵\n",
    "    W_q = np.random.randn(d_model, d_model)\n",
    "    W_k = np.random.randn(d_model, d_model)\n",
    "    W_v = np.random.randn(d_model, d_model)\n",
    "    W_cross_q = np.random.randn(d_model, d_model)\n",
    "    W_cross_k = np.random.randn(d_model, d_model)\n",
    "    W_cross_v = np.random.randn(d_model, d_model)\n",
    "\n",
    "    # 编码器输出\n",
    "    encoder_output = transformer_encoder(x_embedded, W_q, W_k, W_v, d_model, num_heads, d_ff, num_layers)\n",
    "\n",
    "    # 解码器输出\n",
    "    decoder_output = transformer_decoder(target_embedded, encoder_output, W_q, W_k, W_v, W_cross_q, W_cross_k, W_cross_v, d_model, num_heads, d_ff, num_layers)\n",
    "\n",
    "    # 映射到词汇表\n",
    "    logits = np.dot(decoder_output, embeddings.T)\n",
    "    probabilities = softmax(logits, axis=-1)\n",
    "\n",
    "    return probabilities\n",
    "\n",
    "# 示例运行\n",
    "x = np.array([[0, 1, 2, 3, 4]])  # 中文输入序列\n",
    "target = np.array([[5, 6, 7, 8, 9]])  # 英文目标序列 (shifted right)\n",
    "\n",
    "output = transformer(x, target, vocab_size, d_model, num_heads, d_ff, num_layers)\n",
    "print(output)\n",
    "\n",
    "# 选择最大概率的词汇索引\n",
    "output_index = np.argmax(output, axis=-1)\n",
    "output_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TF",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
