{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (4,4) (4,3) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_10876\\1749945156.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[1;31m# 初始化模型并训练（注意：这里X没有添加偏置列，因为我们在计算linear_output时直接加了b）\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSoftmaxRegression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_dim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_iters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[1;31m# 预测\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_10876\\1749945156.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m     42\u001b[0m         \u001b[1;31m# 训练模型\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_iters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mgradient_W\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient_b\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_gradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mW\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearning_rate\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mgradient_W\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mb\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearning_rate\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mgradient_b\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_10876\\1749945156.py\u001b[0m in \u001b[0;36mcompute_gradient\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[0mm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[0mproba\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m         \u001b[0mgradient_W\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mproba\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m         \u001b[0mgradient_b\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mproba\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mgradient_W\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient_b\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (4,4) (4,3) "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class SoftmaxRegression:\n",
    "    def __init__(self, input_dim, num_classes, learning_rate=0.01, num_iters=1000):\n",
    "        # 初始化参数\n",
    "        self.W = np.random.randn(num_classes, input_dim) * 0.01  # 随机初始化权重矩阵\n",
    "        self.b = np.zeros(num_classes)                          # 初始化偏置向量\n",
    "        self.learning_rate = learning_rate                      # 学习率\n",
    "        self.num_iters = num_iters                              # 迭代次数\n",
    "\n",
    "    def softmax(self, z):\n",
    "        # Softmax函数，确保数值稳定性\n",
    "        exp_z = np.exp(z - np.max(z, axis=0, keepdims=True))\n",
    "        return exp_z / np.sum(exp_z, axis=0, keepdims=True)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        # 计算预测概率\n",
    "        linear_output = np.dot(X, self.W.T) + self.b  # 注意这里直接使用了偏置向量b\n",
    "        return self.softmax(linear_output)\n",
    "\n",
    "    def predict(self, X):\n",
    "        # 返回预测类别\n",
    "        proba = self.predict_proba(X)\n",
    "        return np.argmax(proba, axis=0)\n",
    "\n",
    "    def compute_loss(self, X, y):\n",
    "        # 计算交叉熵损失\n",
    "        m = X.shape[0]\n",
    "        proba = self.predict_proba(X)\n",
    "        log_proba = np.log(proba + 1e-9)  # 数值稳定性\n",
    "        return -np.sum(y * log_proba) / m\n",
    "\n",
    "    def compute_gradient(self, X, y):\n",
    "        # 计算梯度\n",
    "        m = X.shape[0]\n",
    "        proba = self.predict_proba(X)\n",
    "        gradient_W = np.dot(X.T, (proba - y)) / m\n",
    "        gradient_b = np.sum(proba - y, axis=0) / m\n",
    "        return gradient_W, gradient_b\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # 训练模型\n",
    "        for i in range(self.num_iters):\n",
    "            gradient_W, gradient_b = self.compute_gradient(X, y)\n",
    "            self.W -= self.learning_rate * gradient_W\n",
    "            self.b -= self.learning_rate * gradient_b\n",
    "            if i % 100 == 0:  # 每100次迭代打印一次损失\n",
    "                loss = self.compute_loss(X, y)\n",
    "                print(f\"Iteration {i}: Loss = {loss}\")\n",
    "\n",
    "# 示例数据\n",
    "# 假设有4个样本，每个样本有3个特征，类别数为3\n",
    "X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]])  # 特征矩阵\n",
    "y = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1], [1, 0, 0]])     # 独热编码的标签矩阵\n",
    "\n",
    "# 初始化模型并训练（注意：这里X没有添加偏置列，因为我们在计算linear_output时直接加了b）\n",
    "model = SoftmaxRegression(input_dim=X.shape[1], num_classes=y.shape[0], learning_rate=0.1, num_iters=1000)\n",
    "model.fit(X, y)\n",
    "\n",
    "# 预测\n",
    "predictions = model.predict(X)\n",
    "print(\"Predictions:\", predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkAAAAHFCAYAAAAaD0bAAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/d3fzzAAAACXBIWXMAAA9hAAAPYQGoP6dpAABTsUlEQVR4nO3deXwTZf4H8M8kaZJeSS96QWnL1VJarpajRQ7lPhRklXoVlFVEFgRZV0FwBVat+HORRQHFXUHRBdbl8EKXcoNFjtJy31dLD0qvpHfaZH5/tI3GFmxpm0mbz/v1mlebZ56ZfGdc7WefeWZGEEVRBBEREZEdkUldABEREZG1MQARERGR3WEAIiIiIrvDAERERER2hwGIiIiI7A4DEBEREdkdBiAiIiKyOwxAREREZHcYgIiIiMjuMAARkdnJkyfxzDPPIDg4GGq1Gi4uLujduzfeffdd5OXlNct3vv3229i2bVuz7FtK//73v7F8+fI61wmCgEWLFlm1HiKyJPBVGEQEAJ988glmzJiBkJAQzJgxA2FhYaioqMCxY8fwySefoEePHti6dWuTf6+LiwseeeQRrFu3rsn3LaVx48bh9OnTuH79eq11P//8M9q1a4d27dpZvzAiAgAopC6AiKR36NAhvPDCCxg+fDi2bdsGlUplXjd8+HD8+c9/xo8//ihhhbaptLQUjo6ODd6uf//+zVANETUEL4EREd5++20IgoA1a9ZYhJ8aSqUSDz30kPmzyWTCu+++i9DQUKhUKnh7e2Py5Mm4efOmxXbJyckYN24cvL29oVKp4O/vj7Fjx5r7CYKA4uJifPbZZxAEAYIgYMiQIXetNS8vDzNmzEDbtm2hVCrRoUMHLFiwAOXl5eY+vXr1wsCBA2ttazQa0bZtW0ycONHcZjAY8Oabb5qPpU2bNnjmmWdw+/Zti22DgoIwbtw4bNmyBb169YJarcbixYvrrHHIkCH4/vvvcePGDfNxCYJgXv/bS2Dr1q2DIAjYvXs3nnvuOXh6ekKj0WDy5MkoLi5GVlYWJk2aBDc3N/j5+eHll19GRUWFxXfW9ziIqApHgIjsnNFoxO7duxEZGYmAgIB6bfPCCy9gzZo1mDlzJsaNG4fr16/j9ddfx969e3H8+HF4eXmhuLgYw4cPR3BwMFauXAkfHx9kZWVhz549KCwsBFA18vTAAw/g/vvvx+uvvw4A0Gg0d/zesrIy3H///bhy5QoWL16M7t2748CBA4iPj0dKSgq+//57AMAzzzyD2bNn49KlS+jcubN5+x07diAjIwPPPPMMgKogN378eBw4cACvvPIKYmJicOPGDbzxxhsYMmQIjh07ZjHCc/z4cZw7dw4LFy5EcHAwnJ2d66xz1apVmDZtGq5cudKgy4bPPvssJk6ciI0bNyI5ORmvvfYaKisrceHCBUycOBHTpk3Dzp07sXTpUvj7+2Pu3Ln3dBxEBEAkIruWlZUlAhAfe+yxevU/d+6cCECcMWOGRfvhw4dFAOJrr70miqIoHjt2TAQgbtu27a77c3Z2FqdMmVKv7/7oo49EAOJ//vMfi/alS5eKAMQdO3aIoiiKOTk5olKpNNdSY9KkSaKPj49YUVEhiqIobtiwQQQgbt682aLf0aNHRQDiqlWrzG2BgYGiXC4XL1y4UK9ax44dKwYGBta5DoD4xhtvmD+vXbtWBCDOmjXLot+ECRNEAOKyZcss2nv27Cn27t3b/Lkhx0FEVXgJjIgaZM+ePQCAp59+2qK9b9++6Nq1K3bt2gUA6NSpE9zd3fHqq6/io48+wtmzZxv93bt374azszMeeeQRi/aaWmq+29PTEw8++CA+++wzmEwmAEB+fj6+/vprTJ48GQpF1eD3d999Bzc3Nzz44IOorKw0Lz179oSvry/27t1r8T3du3dHly5dGn0cdzJu3DiLz127dgUAjB07tlb7jRs3zJ8behxExDlARHbPy8sLTk5OuHbtWr365+bmAgD8/PxqrfP39zev12q12LdvH3r27InXXnsN3bp1g7+/P954441a81fqKzc3F76+vhbzaQDA29sbCoXC/N0AMHXqVKSnpyMhIQEAsGHDBpSXl1sEt1u3bqGgoABKpRIODg4WS1ZWFnJyciy+p65jbkoeHh4Wn5VK5R3by8rKzJ8behxExDlARHZPLpdj6NCh+OGHH3Dz5s3fvTXb09MTAJCZmVmrb0ZGBry8vMyfIyIisHHjRoiiiJMnT2LdunVYsmQJHB0dMW/evAbX6unpicOHD0MURYsQlJ2djcrKSovvHjlyJPz9/bF27VqMHDkSa9euRb9+/RAWFmbu4+XlBU9Pzzve4ebq6mrx+bfBy1Y09DiIiCNARARg/vz5EEURzz33HAwGQ631FRUV+PbbbwEADzzwAADgiy++sOhz9OhRnDt3DkOHDq21vSAI6NGjB95//324ubnh+PHj5nUqlQqlpaX1qnPo0KEoKiqq9eDEzz//3Ly+hlwuR1xcHLZt24YDBw7g2LFjmDp1qsV248aNQ25uLoxGI6KiomotISEh9aqrLg05rsZqzuMgaq04AkREiI6OxurVqzFjxgxERkbihRdeQLdu3VBRUYHk5GSsWbMG4eHhePDBBxESEoJp06bhgw8+gEwmw+jRo813gQUEBOCll14CUDUvZdWqVZgwYQI6dOgAURSxZcsWFBQUYPjw4ebvjoiIwN69e/Htt9/Cz88Prq6ud/yDPXnyZKxcuRJTpkzB9evXERERgYMHD+Ltt9/GmDFjMGzYMIv+U6dOxdKlS/HEE0/A0dERsbGxFusfe+wxfPnllxgzZgxmz56Nvn37wsHBATdv3sSePXswfvx4PPzww/d0TiMiIrBlyxasXr0akZGRkMlkiIqKuqd9/Z7mPA6iVkvaOdhEZEtSUlLEKVOmiO3btxeVSqXo7Ows9urVS/zrX/8qZmdnm/sZjUZx6dKlYpcuXUQHBwfRy8tLfOqpp8S0tDRzn/Pnz4uPP/642LFjR9HR0VHUarVi3759xXXr1tX6zgEDBohOTk4iAHHw4MF3rTE3N1ecPn266OfnJyoUCjEwMFCcP3++WFZWVmf/mJgYEYD45JNP1rm+oqJCfO+998QePXqIarVadHFxEUNDQ8Xnn39evHTpkrlfYGCgOHbs2N87hWZ5eXniI488Irq5uYmCIIi//s8t7nAX2NGjRy328cYbb4gAxNu3b1u0T5kyRXR2dr6n4yCiKnwVBhEREdkdzgEiIiIiu8MARERERHaHAYiIiIjsDgMQERER2R0GICIiIrI7DEBERERkd/ggxDqYTCZkZGTA1dXVZh99T0RERJZEUURhYSH8/f0hk919jIcBqA4ZGRkICAiQugwiIiK6B2lpab/7XkMGoDrUvDgwLS0NGo1G4mqIiIioPvR6PQICAur1AmAGoDrUXPbSaDQMQERERC1MfaavcBI0ERER2R0GICIiIrI7DEBERERkdxiAiIiIyO4wABEREZHdYQAiIiIiu8MARERERHaHAYiIiIjsDgMQERER2R0GICIiIrI7DEBERERkdxiAiIiIyO4wAFlRhdGELF0Z0vJKpC6FiIjIrjEAWdGx6/noH78LT689InUpREREdo0ByIo0jgoAgL6sUuJKiIiI7BsDkBVp1A4AgMKyCokrISIism8MQFZUE4DKKkworzRKXA0REZH9YgCyIhe1wvx7IS+DERERSYYByIrkMgEuqqoQxABEREQkHQYgK3OtHgXSl3IeEBERkVQYgKzsl4nQHAEiIiKSCgOQlf1yKzxHgIiIiKTCAGRlrrwVnoiISHIMQFamMc8B4iUwIiIiqTAAWRlHgIiIiKTHAGRlfB0GERGR9BiArKzmLjBOgiYiIpIOA5CV1VwC4xwgIiIi6TAAWRlvgyciIpIeA5CVufJBiERERJJjALIyDV+FQUREJDnJA9CqVasQHBwMtVqNyMhIHDhw4K79y8vLsWDBAgQGBkKlUqFjx4749NNPLfps3rwZYWFhUKlUCAsLw9atW5vzEBpE48jb4ImIiKQmaQDatGkT5syZgwULFiA5ORkDBw7E6NGjkZqaesdtJk2ahF27duFf//oXLly4gA0bNiA0NNS8/tChQ4iNjUVcXBxOnDiBuLg4TJo0CYcPH7bGIf2umpehFpZXwmQSJa6GiIjIPgmiKEr2V7hfv37o3bs3Vq9ebW7r2rUrJkyYgPj4+Fr9f/zxRzz22GO4evUqPDw86txnbGws9Ho9fvjhB3PbqFGj4O7ujg0bNtSrLr1eD61WC51OB41G08CjuruyCiNCX/8RAHBq0QjznCAiIiJqnIb8/ZZsBMhgMCApKQkjRoywaB8xYgQSExPr3Oabb75BVFQU3n33XbRt2xZdunTByy+/jNLSUnOfQ4cO1drnyJEj77hPoOqyml6vt1iai9pBDqW86rTzYYhERETSUEj1xTk5OTAajfDx8bFo9/HxQVZWVp3bXL16FQcPHoRarcbWrVuRk5ODGTNmIC8vzzwPKCsrq0H7BID4+HgsXry4kUdUfxpHBXKKDNXzgByt9r1ERERURfJJ0IIgWHwWRbFWWw2TyQRBEPDll1+ib9++GDNmDJYtW4Z169ZZjAI1ZJ8AMH/+fOh0OvOSlpbWiCP6fTVPgy4o4URoIiIiKUg2AuTl5QW5XF5rZCY7O7vWCE4NPz8/tG3bFlqt1tzWtWtXiKKImzdvonPnzvD19W3QPgFApVJBpVI14mgaRutUFYB0vBWeiIhIEpKNACmVSkRGRiIhIcGiPSEhATExMXVuM2DAAGRkZKCoqMjcdvHiRchkMrRr1w4AEB0dXWufO3bsuOM+peBWfSu8jiNAREREkpD0EtjcuXPxz3/+E59++inOnTuHl156CampqZg+fTqAqktTkydPNvd/4okn4OnpiWeeeQZnz57F/v378Ze//AVTp06Fo2PVXJrZs2djx44dWLp0Kc6fP4+lS5di586dmDNnjhSHWCc3JyUAoKDUIHElRERE9kmyS2BA1S3rubm5WLJkCTIzMxEeHo7t27cjMDAQAJCZmWnxTCAXFxckJCRg1qxZiIqKgqenJyZNmoQ333zT3CcmJgYbN27EwoUL8frrr6Njx47YtGkT+vXrZ/XjuxOtI+cAERERSUnS5wDZquZ8DhAALN95Ect3XsIT/drj7Ycjmnz/RERE9qhFPAfInnEOEBERkbQYgCTAOUBERETSYgCSQM1t8JwDREREJA0GIAm4cRI0ERGRpBiAJFBzCYwPQiQiIpIGA5AEakaAisorUWE0SVwNERGR/WEAkoCmOgABHAUiIiKSAgOQBOQyARp11TMoOQ+IiIjI+hiAJPLLPCDeCk9ERGRtDEASceOt8ERERJJhAJII3wdGREQkHQYgifzyNGgGICIiImtjAJLIL+8D4xwgIiIia2MAkkjNHKB8XgIjIiKyOgYgibhXXwLL4wgQERGR1TEAScTTpToAFTEAERERWRsDkEQ8nKsDUDEDEBERkbUxAEmkJgDlMgARERFZHQOQRDydVQCA/BIDTCZR4mqIiIjsCwOQRNydq+4CM5pEFJZVSlwNERGRfWEAkohKIYeLquqFqLnF5RJXQ0REZF8YgCTEidBERETSYACSECdCExERSYMBSEKeHAEiIiKSBAOQhHgJjIiISBoMQBJiACIiIpIGA5CEGICIiIikwQAkIU6CJiIikgYDkIRqXoiazwBERERkVQxAEvKofh0GL4ERERFZFwOQhGpug79dVA5R5PvAiIiIrIUBSEJtXKtGgAyVJuj5PjAiIiKrYQCSkNpBDld11fvAbhfyfWBERETWwgAksZpRIAYgIiIi62EAklgbl+oAVMQAREREZC0MQBLz1qgBcASIiIjImhiAJGYeAWIAIiIishoGIInVzAHKLiyTuBIiIiL7wQAkMU6CJiIisj4GIIkxABEREVkfA5DEauYA5fAuMCIiIqthAJJYzQhQbrEBlUaTxNUQERHZBwYgiXk4KyETAFGsCkFERETU/BiAJCaXCfDirfBERERWJXkAWrVqFYKDg6FWqxEZGYkDBw7cse/evXshCEKt5fz58+Y+69atq7NPWZnt3mbOW+GJiIisSyHll2/atAlz5szBqlWrMGDAAHz88ccYPXo0zp49i/bt299xuwsXLkCj0Zg/t2nTxmK9RqPBhQsXLNrUanXTFt+EfDVqnMnQI0vHESAiIiJrkDQALVu2DH/84x/x7LPPAgCWL1+O//3vf1i9ejXi4+PvuJ23tzfc3NzuuF4QBPj6+jZ1uc3GV1sVzrJ0pRJXQkREZB8kuwRmMBiQlJSEESNGWLSPGDECiYmJd922V69e8PPzw9ChQ7Fnz55a64uKihAYGIh27dph3LhxSE5Ovuv+ysvLodfrLRZr8qsOQJk6XgIjIiKyBskCUE5ODoxGI3x8fCzafXx8kJWVVec2fn5+WLNmDTZv3owtW7YgJCQEQ4cOxf79+819QkNDsW7dOnzzzTfYsGED1Go1BgwYgEuXLt2xlvj4eGi1WvMSEBDQNAdZT75aRwBAlp4BiIiIyBokvQQGVF2u+jVRFGu11QgJCUFISIj5c3R0NNLS0vDee+9h0KBBAID+/fujf//+5j4DBgxA79698cEHH2DFihV17nf+/PmYO3eu+bNer7dqCOIIEBERkXVJNgLk5eUFuVxea7QnOzu71qjQ3fTv3/+uozsymQx9+vS5ax+VSgWNRmOxWNMvc4AYgIiIiKxBsgCkVCoRGRmJhIQEi/aEhATExMTUez/Jycnw8/O743pRFJGSknLXPlLz1VQFoKLyShSWVUhcDRERUesn6SWwuXPnIi4uDlFRUYiOjsaaNWuQmpqK6dOnA6i6NJWeno7PP/8cQNVdYkFBQejWrRsMBgO++OILbN68GZs3bzbvc/Hixejfvz86d+4MvV6PFStWICUlBStXrpTkGOvDWaWARq2AvqwSWboyuKodpC6JiIioVZM0AMXGxiI3NxdLlixBZmYmwsPDsX37dgQGBgIAMjMzkZqaau5vMBjw8ssvIz09HY6OjujWrRu+//57jBkzxtynoKAA06ZNQ1ZWFrRaLXr16oX9+/ejb9++Vj++hvDTOkJfVohMXRk6+7hKXQ4REVGrJoiiKEpdhK3R6/XQarXQ6XRWmw805dMj2HfxNt79Q3dM6mPdu9CIiIhag4b8/Zb8VRhUhXeCERERWQ8DkI3wNQcgPg2aiIiouTEA2Qh/t6qHIaYXMAARERE1NwYgG9HOvSoA3cxnACIiImpuDEA2IsDdCQCQnl8Kk4nz0omIiJoTA5CN8NOqIZcJMBhNuF1ULnU5RERErRoDkI1QyGXmJ0Kn5ZVIXA0REVHrxgBkQwI8OA+IiIjIGhiAbEi76nlAHAEiIiJqXgxANoR3ghEREVkHA5ANqbkT7GYBR4CIiIiaEwOQDakZAUrL4wgQERFRc2IAsiEBHlUjQBkFpag0miSuhoiIqPViALIhvho1lAoZKk0iX4lBRETUjBiAbIhMJiDIs2oU6FpOscTVEBERtV4MQDYm2MsZAHCdAYiIiKjZMADZmKDqAMQRICIioubDAGRjgj2rA1Aub4UnIiJqLgxANibYPAJUJHElRERErRcDkI2pCUDp+aUwVPJWeCIioubAAGRj2riq4KyUwyQCqXwnGBERUbNgALIxgiBwIjQREVEzYwCyQR3buAAALmdzHhAREVFzYACyQV18qgLQpVuFEldCRETUOjEA2aBO3q4AgIvZDEBERETNgQHIBtWMAF3OLoLJJEpcDRERUevDAGSDAj2doVTIUFZhws18vhSViIioqTEA2SC5TDBPhL7IeUBERERNjgHIRnX2rg5AnAdERETU5BiAbNQvd4LxVngiIqKmxgBko0J9NQCAc5l6iSshIiJqfRiAbFR4Wy0A4FJ2EcoqjBJXQ0RE1LowANkoH40Kns5KGE0izmdxHhAREVFTYgCyUYIgoFv1KNDpdJ3E1RAREbUuDEA2LNy/ah7QmQwGICIioqbEAGTDws0jQJwITURE1JQYgGxYuH9VALqQVYgKo0niaoiIiFoPBiAbFuDhCFe1Agajic8DIiIiakIMQDZMEAR0q54HdJrzgIiIiJoMA5CNi6ieB5SSViBtIURERK0IA5CNiwx0BwAcv5EvcSVEREStBwOQjetdHYAu3CqEvqxC4mqIiIhaBwYgG+ftqkZ7DyeIIpCcWiB1OURERK0CA1ALUHMZLImXwYiIiJqE5AFo1apVCA4OhlqtRmRkJA4cOHDHvnv37oUgCLWW8+fPW/TbvHkzwsLCoFKpEBYWhq1btzb3YTQrzgMiIiJqWpIGoE2bNmHOnDlYsGABkpOTMXDgQIwePRqpqal33e7ChQvIzMw0L507dzavO3ToEGJjYxEXF4cTJ04gLi4OkyZNwuHDh5v7cJpNTQBKTs1HJR+ISERE1GiCKIqiVF/er18/9O7dG6tXrza3de3aFRMmTEB8fHyt/nv37sX999+P/Px8uLm51bnP2NhY6PV6/PDDD+a2UaNGwd3dHRs2bKhXXXq9HlqtFjqdDhqNpmEH1QyMJhE9F+9AYXklvpt1n/kVGURERPSLhvz9lmwEyGAwICkpCSNGjLBoHzFiBBITE++6ba9eveDn54ehQ4diz549FusOHTpUa58jR4783X3aMrlMMN8NdvhansTVEBERtXySBaCcnBwYjUb4+PhYtPv4+CArK6vObfz8/LBmzRps3rwZW7ZsQUhICIYOHYr9+/eb+2RlZTVonwBQXl4OvV5vsdiaAZ08AQA/Xc6RuBIiIqKWTyF1AYIgWHwWRbFWW42QkBCEhISYP0dHRyMtLQ3vvfceBg0adE/7BID4+HgsXrz4Xsq3mgGdvAAAh6/mosJogoNc8vnrRERELZZkf0W9vLwgl8trjcxkZ2fXGsG5m/79++PSpUvmz76+vg3e5/z586HT6cxLWlpavb/fWrr6auDhrESxwYgTfC0GERFRo0gWgJRKJSIjI5GQkGDRnpCQgJiYmHrvJzk5GX5+fubP0dHRtfa5Y8eOu+5TpVJBo9FYLLZGJhMQ3bHqMthBXgYjIiJqFEkvgc2dOxdxcXGIiopCdHQ01qxZg9TUVEyfPh1A1chMeno6Pv/8cwDA8uXLERQUhG7dusFgMOCLL77A5s2bsXnzZvM+Z8+ejUGDBmHp0qUYP348vv76a+zcuRMHDx6U5Bib0oCOXvj+ZCYSL+dizjCpqyEiImq5JA1AsbGxyM3NxZIlS5CZmYnw8HBs374dgYGBAIDMzEyLZwIZDAa8/PLLSE9Ph6OjI7p164bvv/8eY8aMMfeJiYnBxo0bsXDhQrz++uvo2LEjNm3ahH79+ln9+JrafdXzgI6n5qOovBIuKsmncBEREbVIkj4HyFbZ2nOAfm3w/+3BjdwSfPRUJEaF+0pdDhERkc1oEc8BonvzQKg3AGDXuVsSV0JERNRyMQC1MMO6Vt3NtudCNkwmDt4RERHdCwagFqZPkAdcVQrkFBlw4maB1OUQERG1SAxALYxSIcOgkDYAgF3nsiWuhoiIqGViAGqBhlbPA9rJeUBERET3hAGoBbo/xBsKmYDzWYW4ertI6nKIiIhaHAagFsjdWWl+N9j3JzMlroaIiKjlYQBqocZ2r3r9x3cMQERERA3GANRCjQzzhYNcwIVbhbh0q1DqcoiIiFoUBqAWSuvkgEGdq+4G+5ajQERERA3CANSC1VwG+/5kBvhGEyIiovpjAGrBhof5QKmQ4crtYpzJ0EtdDhERUYvBANSCuaodMLz61Rj/TbopcTVEREQtBwNQC/doVDsAwLaUdJRXGiWuhoiIqGVgAGrhBnZuA1+NGgUlFdh5lq/GICIiqg8GoBZOLhPwh8i2AICvktIkroaIiKhlYABqBR6NDAAA7L94G1m6MomrISIisn33FICWLFmCkpKSWu2lpaVYsmRJo4uihgnyckbfYA+YROA/xzgKRERE9HvuKQAtXrwYRUW1X8JZUlKCxYsXN7ooargn+7UHAPz7cCoqjCaJqyEiIrJt9xSARFGEIAi12k+cOAEPD49GF0UNNzrcD14uKmTpy5Bw9pbU5RAREdk0RUM6u7u7QxAECIKALl26WIQgo9GIoqIiTJ8+vcmLpN+nVMjwRN8ArNh9GZ8lXseYCD+pSyIiIrJZDQpAy5cvhyiKmDp1KhYvXgytVmtep1QqERQUhOjo6CYvkurniX6BWLn3Cg5fy8P5LD1CfTVSl0RERGSTGhSApkyZAgAIDg7GgAEDoFA0aHNqZr5aNUZ188X3pzLx+aEbePvhCKlLIiIiskn3NAfI1dUV586dM3/++uuvMWHCBLz22mswGAxNVhw13OToQADA1uPp0JVUSFwNERGRbbqnAPT888/j4sWLAICrV68iNjYWTk5O+Oqrr/DKK680aYHUMH2DPdDVT4PSCiPW/3xd6nKIiIhs0j0FoIsXL6Jnz54AgK+++gqDBw/Gv//9b6xbtw6bN29uyvqogQRBwPODOgAA1iVeR1kF3w9GRET0W/d8G7zJVPWsmZ07d2LMmDEAgICAAOTk5DRddXRPxnb3Q1s3R+QUGfiWeCIiojrcUwCKiorCm2++ifXr12Pfvn0YO3YsAODatWvw8fFp0gKp4RzkMjw7MBgA8MmBqzCaRIkrIiIisi33FICWL1+O48ePY+bMmViwYAE6deoEAPjvf/+LmJiYJi2Q7k1snwC4OTngRm4JfjydJXU5RERENkUQRbHJhgfKysogl8vh4ODQVLuUhF6vh1arhU6ng0bTcp+lsyzhIlbsuoTwthp8O/O+Op/eTURE1Fo05O93ox7kk5SUhHPnzkEQBHTt2hW9e/duzO6oiT0dE4R/HriK0+l67DqXjWFhvDxJREQE3GMAys7ORmxsLPbt2wc3NzeIogidTof7778fGzduRJs2bZq6TroHHs5KTI4Owkf7rmD5rosY2tWbo0BERES4xzlAs2bNQmFhIc6cOYO8vDzk5+fj9OnT0Ov1ePHFF5u6RmqEaYM6wEkpN48CERER0T0GoB9//BGrV69G165dzW1hYWFYuXIlfvjhhyYrjhrPw1mJKTFBAIDluy6iCad8ERERtVj3FIBMJlOdE50dHBzMzwci2/HcwA5wrh4F2slRICIionsLQA888ABmz56NjIwMc1t6ejpeeuklDB06tMmKo6bx61Gg9xMuwsTnAhERkZ27pwD04YcforCwEEFBQejYsSM6deqE4OBgFBYW4oMPPmjqGqkJPDewA1xUCpzN1OO7U5lSl0NERCSpe7oLLCAgAMePH0dCQgLOnz8PURQRFhaGYcOGNXV91ETcnZWYNqgDliVcxHv/u4BR3XyhVNxT/iUiImrxGvQXcPfu3QgLC4NerwcADB8+HLNmzcKLL76IPn36oFu3bjhw4ECzFEqN9+zAYLRxVSE1rwT/PnxD6nKIiIgk06AAtHz5cjz33HN1Pl1Rq9Xi+eefx7Jly5qsOGpaTkoF5gzrDABYsfsyCssqJK6IiIhIGg0KQCdOnMCoUaPuuH7EiBFISkpqdFHUfCZFBaCDlzPyig1Ys/+q1OUQERFJokEB6NatW3d9z5dCocDt27cbXRQ1Hwe5DK+MCgEA/PPANWTryySuiIiIyPoaFIDatm2LU6dO3XH9yZMn4efn1+iiqHmN7OaLXu3dUFphxLKEi1KXQ0REZHUNCkBjxozBX//6V5SV1R41KC0txRtvvIFx48Y1WXHUPARBwIIxVU/x3nQsDafTdRJXREREZF2C2IB3I9y6dQu9e/eGXC7HzJkzERISAkEQcO7cOaxcuRJGoxHHjx+Hj0/Lfuu4Xq+HVquFTqerc8J3a/HihmR8cyIDfYLc8Z/no/miVCIiatEa8ve7QSNAPj4+SExMRHh4OObPn4+HH34YEyZMwGuvvYbw8HD89NNPDQ4/q1atQnBwMNRqNSIjI+t9G/1PP/0EhUKBnj17WrSvW7cOgiDUWuoatbJ380aHQu0gw9Hr+fj2JB+OSERE9qPBT8ILDAzE9u3bkZOTg8OHD+Pnn39GTk4Otm/fjqCgoAbta9OmTZgzZw4WLFiA5ORkDBw4EKNHj0Zqaupdt9PpdJg8efIdX7uh0WiQmZlpsajV6gbVZg/83RwxY0gnAED89nMoNRglroiIiMg67vlRwO7u7ujTpw/69u0Ld3f3e9rHsmXL8Mc//hHPPvssunbtiuXLlyMgIACrV6++63bPP/88nnjiCURHR9e5XhAE+Pr6WixUt2mDOqCtmyMydWVYve+K1OUQERFZhWTvQjAYDEhKSsKIESMs2keMGIHExMQ7brd27VpcuXIFb7zxxh37FBUVITAwEO3atcO4ceOQnJx811rKy8uh1+stFnuhdpBjwdiqCdEf77uCtLwSiSsiIiJqfpIFoJycHBiNxlpzhnx8fJCVlVXnNpcuXcK8efPw5ZdfQqGo+zVmoaGhWLduHb755hts2LABarUaAwYMwKVLl+5YS3x8PLRarXkJCAi49wNrgUaH+yK6gyfKK01445szaMC8eCIiohZJ8rdh/vbOI1EU67wbyWg04oknnsDixYvRpUuXO+6vf//+eOqpp9CjRw8MHDgQ//nPf9ClS5e7vqV+/vz50Ol05iUtLe3eD6gFEgQBf5sQDge5gN3ns/Hj6boDKBERUWshWQDy8vKCXC6vNdqTnZ1d551khYWFOHbsGGbOnAmFQgGFQoElS5bgxIkTUCgU2L17d53fI5PJ0KdPn7uOAKlUKmg0GovF3nTydsH0wR0BAIu+PYOi8kqJKyIiImo+kgUgpVKJyMhIJCQkWLQnJCQgJiamVn+NRoNTp04hJSXFvEyfPh0hISFISUlBv3796vweURSRkpLCJ1TXw5/u74RATyfc0pfj7zsuSF0OERFRs6l7Io2VzJ07F3FxcYiKikJ0dDTWrFmD1NRUTJ8+HUDVpan09HR8/vnnkMlkCA8Pt9je29sbarXaon3x4sXo378/OnfuDL1ejxUrViAlJQUrV6606rG1RGoHOf42PhyTPz2CzxKv4w+92yG8rVbqsoiIiJqcpAEoNjYWubm5WLJkCTIzMxEeHo7t27cjMDAQAJCZmfm7zwT6rYKCAkybNg1ZWVnQarXo1asX9u/fj759+zbHIbQ6g7q0wYM9/PHtiQy8tvUUts4YALmMT4gmIqLWpUGvwrAX9vIqjDvJLizD0L/vQ2FZJRaO7YpnB3aQuiQiIqLf1WyvwiD74O2qxvzRVc8Gem/HBVzPKZa4IiIioqbFAER1erxvAAZ08kRZhQmvbD4Jk4kDhURE1HowAFGdBEHAOxO7w0kpx5FreVj/8w2pSyIiImoyDEB0RwEeTpg3OhQAsPTH83xNBhERtRoMQHRXT/ULRN9gD5QYjHh180m+JoOIiFoFBiC6K5lMwLt/6A61gwyJV3J5KYyIiFoFBiD6XUFeznh1VNWlsLe+P4fL2YUSV0RERNQ4DEBUL1OigzCwsxfKK02YsykFhkqT1CURERHdMwYgqheZTMB7j/aAm5MDTqfrsXznRalLIiIiumcMQFRvPho13pkYAQBYve8KjlzLk7giIiKie8MARA0yKtwPj0a2gygCL21Kgb6sQuqSiIiIGowBiBrsjYe6ob2HE9ILSvHXbad5azwREbU4DEDUYC4qBd6P7QG5TMC2lAx8deym1CURERE1CAMQ3ZPIQA/MHd4FAPDXb07jQhZvjSciopaDAYju2QuDO2JQlzYoqzBhxpdJKC6vlLokIiKiemEAonsmkwl4f1IP+GhUuHK7GK9zPhAREbUQDEDUKJ4uKqx4rBdkArAlOR1fJXE+EBER2T4GIGq0fh088ecRIQCAv359Guez9BJXREREdHcMQNQkfj0f6Pn1SdCV8PlARERkuxiAqEnIZAL+EdsT7dwdcSO3BLM3JcNo4nwgIiKyTQxA1GTcnZX4OC4SagcZ9l64jWUJF6QuiYiIqE4MQNSkuvlrsfQP3QEAK/dcwQ+nMiWuiIiIqDYGIGpy43u2xbP3BQMA/vzVCVy8xYckEhGRbWEAomYxb3QoYjp6osRgxHOfH0N+sUHqkoiIiMwYgKhZKOQyfPhEb/Ok6OlfJMFQaZK6LCIiIgAMQNSMPJyV+PTpPnBVKXD4Wh5e23qKT4omIiKbwABEzaqLjys+fLI3ZALw36SbWL3vitQlERERMQBR8xvcpQ0WPdQNAPDujxd4ZxgREUmOAYisYnJ0EJ6OCQIAvPSfFKSkFUhaDxER2TcGILKahWO7YkhI1esypq47iqu3i6QuiYiI7BQDEFlNzZ1hEW21yCs2YPKnR5CtL5O6LCIiskMMQGRVLioF1j7TB0GeTriZX4opa49CX8YXpxIRkXUxAJHVebmo8PnUfvByUeFcph7T1yehvNIodVlERGRHGIBIEu09nbDumT5wUSmQeCUXc/9zgm+PJyIiq2EAIsmEt9Xio6ci4SAX8P3JTCzcdpoPSiQiIqtgACJJ3dfZC+/H9oRMADYcScWS784yBBERUbNjACLJjevuj6V/6A4AWPvTdby344LEFRERUWvHAEQ24dGoAPxtQjgAYOWeK/hg1yWJKyIiotaMAYhsRlz/QCwc2xUA8PeEi/hk/1WJKyIiotaKAYhsyrMDO+DPw7sAAN7afg7/OnhN4oqIiKg1YgAimzPzgU740/0dAQB/++4sPuYb5ImIqIkxAJHNEQQBL48IwYtDOwMA4n84j5V7LktcFRERtSYMQGSTBEHA3OFdzJfD/u9/F/B+wkXeIk9ERE2CAYhs2qyhnfHqqFAAwD92XcL//e8CQxARETWa5AFo1apVCA4OhlqtRmRkJA4cOFCv7X766ScoFAr07Nmz1rrNmzcjLCwMKpUKYWFh2Lp1axNXTdb0wpCO5rvDVu29gkXfnIGJr80gIqJGkDQAbdq0CXPmzMGCBQuQnJyMgQMHYvTo0UhNTb3rdjqdDpMnT8bQoUNrrTt06BBiY2MRFxeHEydOIC4uDpMmTcLhw4eb6zDICp4d2AF/G98NggB8dugG5mxKgaHSJHVZRETUQgmihNcT+vXrh969e2P16tXmtq5du2LChAmIj4+/43aPPfYYOnfuDLlcjm3btiElJcW8LjY2Fnq9Hj/88IO5bdSoUXB3d8eGDRvqVZder4dWq4VOp4NGo2n4gVGz+TolHX/+zwlUmkQM6tIGHz3VG05KhdRlERGRDWjI32/JRoAMBgOSkpIwYsQIi/YRI0YgMTHxjtutXbsWV65cwRtvvFHn+kOHDtXa58iRI++6z/Lycuj1eouFbNP4nm3xzylRcHSQY//F23jyn4dRUGKQuiwiImphJAtAOTk5MBqN8PHxsWj38fFBVlZWndtcunQJ8+bNw5dffgmFou7/15+VldWgfQJAfHw8tFqteQkICGjg0ZA1DQnxxhfP9oPW0QHJqQV49KNDyNSVSl0WERG1IJJPghYEweKzKIq12gDAaDTiiSeewOLFi9GlS5cm2WeN+fPnQ6fTmZe0tLQGHAFJITLQHV9Nj4avRo1L2UV4eGUizmZw5I6IiOpHsgDk5eUFuVxea2QmOzu71ggOABQWFuLYsWOYOXMmFAoFFAoFlixZghMnTkChUGD37t0AAF9f33rvs4ZKpYJGo7FYyPZ18XHFf1+IRidvF2Tpy/DoR4nYcyFb6rKIiKgFkCwAKZVKREZGIiEhwaI9ISEBMTExtfprNBqcOnUKKSkp5mX69OkICQlBSkoK+vXrBwCIjo6utc8dO3bUuU9q+dq5O2Hz9BjEdPREscGIP647ivU/35C6LCIisnGS3j4zd+5cxMXFISoqCtHR0VizZg1SU1Mxffp0AFWXptLT0/H5559DJpMhPDzcYntvb2+o1WqL9tmzZ2PQoEFYunQpxo8fj6+//ho7d+7EwYMHrXpsZD1aJwese6YvXtt6Cv9NuonXt53GjZxizB/TFXLZnS99EhGR/ZI0AMXGxiI3NxdLlixBZmYmwsPDsX37dgQGBgIAMjMzf/eZQL8VExODjRs3YuHChXj99dfRsWNHbNq0yTxCRK2TUiHD/z3SHcFezvi//13APw9eQ2peCd6P7QlnFW+TJyIiS5I+B8hW8TlALds3JzLw8lcnYKg0IcTHFWsmRyLQ01nqsoiIqJm1iOcAETWXh3r4Y8Nz/dHGVYULtwrx0Ic/4cCl21KXRURENoQBiFqlyEB3fDfrPvQMcIOutAJTPj2CNfuv8EWqREQEgAGIWjEfjRobp/XHo5HtYBKBt7efx5xNKSg1GKUujYiIJMYARK2a2kGOdx/pjsUPdYNcJuDrlAw8vOonXLldJHVpREQkIQYgavUEQcCUmCB88cd+8HJR4XxWIR764CC+OZEhdWlERCQRBiCyG9EdPbH9xfvQv4MHig1GvLghGQu3nUJZBS+JERHZGwYgsiveGjW++GM/zHqgEwDgi59T8YfVibiRWyxxZUREZE0MQGR3FHIZ/jwiBOue6QN3JwecydBj3ApeEiMisicMQGS3hoR4Y/vsgYgKdEdheSVe3JCMlzalQF9WIXVpRETUzBiAyK75aR2xcVp/zB7aGTIB2JqcjtHLD+Do9TypSyMiombEAER2TyGX4aXhXfDV9GgEeDgivaAUsR8fwnv/u4AKo0nq8oiIqBkwABFViwz0wPYXB2Ji77YwicCHey7jkdWJuJxdKHVpRETUxBiAiH7FVe2AZZN64sMnekGjVuDETR3GrDiI1XuvoJKjQURErQYDEFEdxnX3x/9eGoQhIW1gqDRh6Y/n8YfVibh4i6NBREStAQMQ0R34aR2x9uk++L9HusO1ejRo3IqDWLnnMkeDiIhaOAYgorsQBAGPRgUg4aXBeCDUGwajCf/3vwt4eFUiTqfrpC6PiIjuEQMQUT34atX415Qo/P3RHtCoFTiVrsNDHx7E4m/PoKi8UuryiIiogRiAiOpJEAT8IbIddv55MB7s4Q+TCKz96TqG/X0ffjydCVEUpS6RiIjqiQGIqIG8XdX44PFe+GxqX7T3cEKWvgzTvziOZz87hpv5JVKXR0RE9cAARHSPBndpgx0vDcLM+zvBQS5g1/lsDF+2Hx/uvsQ3zBMR2ThB5Lh9LXq9HlqtFjqdDhqNRupyqAW4nF2I17aexpFrVa/QCPBwxIIxYRjZzQeCIEhcHRGRfWjI328GoDowANG9EEUR35zIQPz288jSlwEABnTyxF/HdUOIr6vE1RERtX4MQI3EAESNUVxeidV7r2DNgaswVJoglwl4ql97vDS8C9yclFKXR0TUajEANRIDEDWFtLwSvPX9Ofx4JgsAoFErMOP+Tng6JghqB7nE1RERtT4MQI3EAERN6afLOfjbd2dxPqvqNRr+WjVeGt4FE3u3g1zG+UFERE2FAaiRGICoqRlNIrYmp2PZjgvI0FXNDwr1dcWro0MxpEsbTpQmImoCDECNxABEzaWswojPEq9j5Z7L0JdVPUE6uoMn/jIqBL3bu0tcHRFRy8YA1EgMQNTcCkoMWLX3CtYlXoehsurFqkNC2uClYV3QI8BN2uKIiFooBqBGYgAia7mZX4IVuy5h8/F0GE1V/yoODfXGnGFdENFOK3F1REQtCwNQIzEAkbXdyC3Gil2XsTX5JqpzEIaH+WDOsM7o5s8gRERUHwxAjcQARFK5ersIH+y+jK9T0i2C0IwhHdGLc4SIiO6KAaiRGIBIapezi7Bi1yV8ezIDNf+GRnfwxAtDOmJgZy/eNUZEVAcGoEZiACJbcTm7CB/vu4KtyemorB4SCm+rwQuDO2FUuC+fI0RE9CsMQI3EAES2JqOgFP88cA0bjqSitPpN88Fezpg2qAMe7tWWT5YmIgIDUKMxAJGtyis2YF3idXyWeB260goAgLuTA57sF4i46ED4aNQSV0hEJB0GoEZiACJbV1xeiQ1HUrH2p+tILygFADjIBYzr7o+pA4J5Cz0R2SUGoEZiAKKWotJoQsLZW/j0p2s4ej3f3N4nyB1TBwRjeJgPFHKZhBUSEVkPA1AjMQBRS3TyZgE+PXgN353MNE+Y9tOq8Vif9nisbwAvjxFRq8cA1EgMQNSS3dKXYf2hG/jy8A3kl1TNE5LLBAzv6oMn+7fHgI5ekPHuMSJqhRiAGokBiFqDsgojfjydhS8P37C4PBbk6YQn+rXHI5EB8HBWSlghEVHTYgBqJAYgam0uZBXi34dvYMvxdBSWV72FXimXYWS4Lx6NbIcBnbz4TCEiavEYgBqJAYhaq+LySnx7IgNfHL6B0+l6c7ufVo0/9G6HRyLbIcjLWcIKiYjuHQNQIzEAkT04dVOHr5LS8HVKhvmZQkDVHWSPRgZgTHc/uKgUElZIRNQwDECNxABE9qSswoid527hq2M3ceDSbfNLWJ2Ucozq5ouHevrjvk5evJ2eiGxeQ/5+S/5ftFWrViE4OBhqtRqRkZE4cODAHfsePHgQAwYMgKenJxwdHREaGor333/fos+6desgCEKtpaysrLkPhahFUjvIMa67Pz6b2heJ84biLyND0MHLGSUGI7Ykp+PptUfR7+1d+OvXp5F0Iw/8/0xE1BpIOr69adMmzJkzB6tWrcKAAQPw8ccfY/To0Th79izat29fq7+zszNmzpyJ7t27w9nZGQcPHsTzzz8PZ2dnTJs2zdxPo9HgwoULFtuq1XwGCtHv8dWq8af7O2HGkI44npqPr1My8P3JTOQWG/D5oRv4/NANtHVzxEM9/TG+pz9CfTlCSkQtk6SXwPr164fevXtj9erV5rauXbtiwoQJiI+Pr9c+Jk6cCGdnZ6xfvx5A1QjQnDlzUFBQcM918RIY0S8qjCb8dDkH35zIwP9OZ6HYYDSv6+LjgrER/hgd4YvO3i4QBN5JRkTSacjfb8lGgAwGA5KSkjBv3jyL9hEjRiAxMbFe+0hOTkZiYiLefPNNi/aioiIEBgbCaDSiZ8+e+Nvf/oZevXrdcT/l5eUoLy83f9br9XfsS2RvHOQyDAnxxpAQb5ROMGL3+Wx8nZKOvRdu4+KtIly8dRHv77yIDm2cMTrcF6PD/dDNX8MwREQ2TbIAlJOTA6PRCB8fH4t2Hx8fZGVl3XXbdu3a4fbt26isrMSiRYvw7LPPmteFhoZi3bp1iIiIgF6vxz/+8Q8MGDAAJ06cQOfOnevcX3x8PBYvXtz4gyJq5RyVcozt7oex3f2gK6nA/85m4cfTWTh4KQdXbxdj5Z4rWLnnCgI8HDE63A+jwn3Rs50bnzxNRDZHsktgGRkZaNu2LRITExEdHW1uf+utt7B+/XqcP3/+jtteu3YNRUVF+PnnnzFv3jx8+OGHePzxx+vsazKZ0Lt3bwwaNAgrVqyos09dI0ABAQG8BEZUT4VlFdh9Phs/nMrC3ovZKKswmdf5atQYFuaNoV19EN3BE2oHuYSVElFr1iIugXl5eUEul9ca7cnOzq41KvRbwcHBAICIiAjcunULixYtumMAkslk6NOnDy5dunTH/alUKqhUqgYeARHVcFU7YHzPthjfsy1KDJXYd+E2fjidhd3ns5GlL8MXP6fii59T4aSU475OXhjW1Qf3h3qjjSv/vSMiaUgWgJRKJSIjI5GQkICHH37Y3J6QkIDx48fXez+iKFqM3tS1PiUlBREREY2ql4jqx0mpwOgIP4yO8ENZhRGJV3Kw81w2dp+rCkM7zt7CjrO3IAhAj3ZuGNa1anQo1NeV84aIyGokvQ1+7ty5iIuLQ1RUFKKjo7FmzRqkpqZi+vTpAID58+cjPT0dn3/+OQBg5cqVaN++PUJDQwFUPRfovffew6xZs8z7XLx4Mfr374/OnTtDr9djxYoVSElJwcqVK61/gER2Tu0gxwOhPngg1AfiBBFnMvTYdS4bu87fwsmbOqSkFSAlrQDv7bgIf60aAzu3waAubXBfJy9onRykLp+IWjFJA1BsbCxyc3OxZMkSZGZmIjw8HNu3b0dgYCAAIDMzE6mpqeb+JpMJ8+fPx7Vr16BQKNCxY0e88847eP755819CgoKMG3aNGRlZUGr1aJXr17Yv38/+vbta/XjI6JfCIKA8LZahLfVYvawzrilL8Pu89nYde4WDl7OQYauDJuOpWHTsTTIBKBHgBsGVQeiHu20fBI1ETUpvgqjDnwOEJF1lVUYcfhaHvZfvI39F2/jUnaRxXqNWoH7OnthUOc2uK+zF9q5O0lUKRHZMr4LrJEYgIiklVFQigOXbmP/xRwcvJxj8bJWAAjwcERMBy9Ed/REdEdP+Gj4pHciYgBqNAYgItthNIk4cbPAPDp04qYORpPlf7Y6tHFGdIeqMNS/gye8XHh3GZE9YgBqJAYgIttVVF6Jo9fz8POVXCReycXpDB1++1+xEB9XRHf0RJ8gD0QFuXOEiMhOMAA1EgMQUcuhK63AkWt5SLySg0NXcnE+q7BWn/YeTogKckefIA/0CXJHxzZ8bxlRa8QA1EgMQEQtV25ROQ5fy8Phq7k4ej0f57L0tUaI3J0cEBlYFYaigjwQ0VYLpYJ3mRG1dAxAjcQARNR66MsqkJxagGPX83D0eh5S0gosXtUBACqFDOFttegZ4GZe2rk7cpSIqIVhAGokBiCi1stQacKZDB2OXc/H0et5OHYjH3nFhlr9vFyUvwpE7ugeoIVGzYczEtkyBqBGYgAish+iKOJqTjFSUgvMT6Y+l6lH5W/uNBMEoGMbF3MoimirRYivK1/uSmRDGIAaiQGIyL6VVRhxJkOH5F+Fopv5pbX6KWQCOnm7IKL6CdfhbTXo6qeBk1LSh+wT2S0GoEZiACKi38opKjePEp24WYAzGfo6L53VjBSF+2vMr/4I89fw8hmRFTAANRIDEBH9HlEUkakrw+l0HU5n6Kt+puuQXVheZ//2Hk4I9XVFqJ8GXat/tvdwglzGidZETYUBqJEYgIjoXmXry3AmQ49T1YHoTIYe6QW1L58BgKODHF18XBDqq0Gon2vVT19XuDsrrVw1UevAANRIDEBE1JTyig04n6nHuaxCnM/U48KtQlzIKkR5panO/j4alTkMdfR2QWdvF3TydoErL6MR3RUDUCMxABFRczOaRFzPLcaF6lB0LqsQ57P0SMure7QIAHw1anT2cUHHNi7o7OOCTm1c0NnHFR4cMSICwADUaAxARCSVovLKqlCUpcelW0W4nF2ES9mFuKWve24RAHg4K9GpepSos3dVQAr2coa/myPnGJFdYQBqJAYgIrI1utIKXLldhMu3inD5dhEu3SrEpeyiOm/Pr6FUyBDk6YQgT2cEt3FGBy9n8+9tXFR80jW1OgxAjcQAREQtRYmhEldvF5tHii7dKsLVnGKk5pbAYKx7jhEAuKgUCPJyQrBX1WhRBy9nBHk5I9jTGVonzjWilokBqJEYgIiopTOaRGQUlOJqTjGu5xTjWk4xruYU41pOEdLzS2G6y3/5NWoF2ns6ob2HEwI8qn7WLP5ujnCQ88WxZJsYgBqJAYiIWrPySiPS8kpw9XYxrudWh6PbVT/v9ByjGjIB8Hdz/CUUeVoGJK2jAy+tkWQYgBqJAYiI7FWpwYi0/BKk5pYgNa9qScv75fc73bpfw1kpR1t3R7R1c4S/m6P597bVv3u7qjkxm5pNQ/5+84U1RERk5qiUo4uPK7r4uNZaZzKJyCkqN4eh1DzLoJRdWI5igxEXbxXh4q2iOvevkAnw1aotQtFvwxJfMEvWwABERET1IpMJ8Nao4a1RIyrIo9b6sgoj0gtKkZ5fioyCUvPv6dW/Z+nKUGkScTO/9K53r7k7OcBX6whfjar6pxp+WjV8tNU/NWpo1ApeaqNGYQAiIqImoXaQo2ObqucQ1cVoEpFdWGYRin4blooNRuSXVCC/pALnMu/8XY4OcnMY+m04qglMni4qXm6jO2IAIiIiq5DLBPhpHeGndURUHetFUYSutAJZ+jJk6spwS1f9U1+GLH0ZsnRVPwtKKlBaYcTV6jvb7kQmAJ4uKni7qtDG9dc/1bU+Oyp52c3eMAAREZFNEAQBbk5KuDkpEep75wmspQZjrVCUpbP8PbuwDCYRuF1Yjtu/c2cbUPVcJG9XFbzuEpQ8XZTwcFJCwccAtAoMQERE1KI4KuUIqn5w451UGk3IKzYguzoA3S4sR3ZhWfXPcvPP7MIylFWYUFReiaLyyruOKNVwc3KAp7MSni4qeLko4emsgoezsup3F5V5naezElpHB8h4Gc4mMQAREVGro5DLzBO270YURRSVV9YKRr8OTDVLfokBJhEoKKlAQUkFrtz+/bAklwnwcFbC01kJL5eqoOTp8svv7k5KuDs5wL36dzcnBz5o0koYgIiIyG4JggBXtQNc1Q7ocIfJ2zWMJhEFJQbkFhuQU1SOvGIDcosMyC0qR06xAXlFBuQWlyO3qGq9vqwSRpP4q8twhfWqyVWlgJuzQ3UgUsLDyQFuTtVhydmhuq0qLFUFJwc4Osh5V1wDMQARERHVg1wmVF3aclHV+Zyk3zJUmpBfUhWGcn8VjnKLq0JTbpEB+SUGFJRUVP0srYAoAoXllSgsr0Ra3p0fFfBbSoXsl1BU/VPrWLVoHH/5/deLxtEBGrXCbuc0MQARERE1A6VCBh9N1a359WE0idCXVoWh/JIK5BdbBqS62wyoMIowVJqqJoDryxpcp4tK8augpPglIKmrw5JT3UFKo3aAUtFywxMDEBERkQ2Qy4SqS1rOynpvI4pi1bOTii1DUUFJBXSlVYu+9Jfff/252GAEAPME8PSC+o841VApZHBVV40kuaoVVb87KuCqcjB/dv3NOk11m9ax6nKeVBiAiIiIWihBEOCiUsBFpUBA7Ydz31WF0QR9aQX0ZZUWAenXIem34almKSyrBACUV5pQXlSOnKLff9TAb4W31eC7WQMbvF1TYQAiIiKyQw5ymXlOU0MZTVV3z+mrw1BhWfXP8prPleZwZV5X9qt1ZRXQqB2a4ajqjwGIiIiIGkQuE8xzge6VySQ2YUUN13JnLxEREVGLJfUDIhmAiIiIyO4wABEREZHdYQAiIiIiu8MARERERHaHAYiIiIjsDgMQERER2R0GICIiIrI7DEBERERkdxiAiIiIyO5IHoBWrVqF4OBgqNVqREZG4sCBA3fse/DgQQwYMACenp5wdHREaGgo3n///Vr9Nm/ejLCwMKhUKoSFhWHr1q3NeQhERETUwkgagDZt2oQ5c+ZgwYIFSE5OxsCBAzF69GikpqbW2d/Z2RkzZ87E/v37ce7cOSxcuBALFy7EmjVrzH0OHTqE2NhYxMXF4cSJE4iLi8OkSZNw+PBhax0WERER2ThBFEXJ3kbWr18/9O7dG6tXrza3de3aFRMmTEB8fHy99jFx4kQ4Oztj/fr1AIDY2Fjo9Xr88MMP5j6jRo2Cu7s7NmzYUK996vV6aLVa6HQ6aDSaBhwRERERSaUhf78lGwEyGAxISkrCiBEjLNpHjBiBxMTEeu0jOTkZiYmJGDx4sLnt0KFDtfY5cuTIeu+TiIiIWj+FVF+ck5MDo9EIHx8fi3YfHx9kZWXdddt27drh9u3bqKysxKJFi/Dss8+a12VlZTV4n+Xl5SgvLzd/1ul0AKqSJBEREbUMNX+363NxS7IAVEMQBIvPoijWavutAwcOoKioCD///DPmzZuHTp064fHHH7/nfcbHx2Px4sW12gMCAupzCERERGRDCgsLodVq79pHsgDk5eUFuVxea2QmOzu71gjObwUHBwMAIiIicOvWLSxatMgcgHx9fRu8z/nz52Pu3LnmzyaTCXl5efD09PzdMNZQer0eAQEBSEtL4/yiZsTzbB08z9bDc20dPM/W0VznWRRFFBYWwt/f/3f7ShaAlEolIiMjkZCQgIcfftjcnpCQgPHjx9d7P6IoWly+io6ORkJCAl566SVz244dOxATE3PHfahUKqhUKos2Nze3etdwLzQaDf/lsgKeZ+vgebYenmvr4Hm2juY4z7838lND0ktgc+fORVxcHKKiohAdHY01a9YgNTUV06dPB1A1MpOeno7PP/8cALBy5Uq0b98eoaGhAKqeC/Tee+9h1qxZ5n3Onj0bgwYNwtKlSzF+/Hh8/fXX2LlzJw4ePGj9AyQiIiKbJGkAio2NRW5uLpYsWYLMzEyEh4dj+/btCAwMBABkZmZaPBPIZDJh/vz5uHbtGhQKBTp27Ih33nkHzz//vLlPTEwMNm7ciIULF+L1119Hx44dsWnTJvTr18/qx0dERES2SdLnANmj8vJyxMfHY/78+bUuu1HT4Xm2Dp5n6+G5tg6eZ+uwhfPMAERERER2R/J3gRERERFZGwMQERER2R0GICIiIrI7DEBERERkdxiArGjVqlUIDg6GWq1GZGQkDhw4IHVJLUp8fDz69OkDV1dXeHt7Y8KECbhw4YJFH1EUsWjRIvj7+8PR0RFDhgzBmTNnLPqUl5dj1qxZ8PLygrOzMx566CHcvHnTmofSosTHx0MQBMyZM8fcxvPcNNLT0/HUU0/B09MTTk5O6NmzJ5KSkszreZ6bRmVlJRYuXIjg4GA4OjqiQ4cOWLJkCUwmk7kPz3XD7d+/Hw8++CD8/f0hCAK2bdtmsb6pzml+fj7i4uKg1Wqh1WoRFxeHgoKCxh+ASFaxceNG0cHBQfzkk0/Es2fPirNnzxadnZ3FGzduSF1aizFy5Ehx7dq14unTp8WUlBRx7NixYvv27cWioiJzn3feeUd0dXUVN2/eLJ46dUqMjY0V/fz8RL1eb+4zffp0sW3btmJCQoJ4/Phx8f777xd79OghVlZWSnFYNu3IkSNiUFCQ2L17d3H27Nnmdp7nxsvLyxMDAwPFp59+Wjx8+LB47do1cefOneLly5fNfXiem8abb74penp6it9995147do18auvvhJdXFzE5cuXm/vwXDfc9u3bxQULFoibN28WAYhbt261WN9U53TUqFFieHi4mJiYKCYmJorh4eHiuHHjGl0/A5CV9O3bV5w+fbpFW2hoqDhv3jyJKmr5srOzRQDivn37RFEURZPJJPr6+orvvPOOuU9ZWZmo1WrFjz76SBRFUSwoKBAdHBzEjRs3mvukp6eLMplM/PHHH617ADausLBQ7Ny5s5iQkCAOHjzYHIB4npvGq6++Kt533313XM/z3HTGjh0rTp061aJt4sSJ4lNPPSWKIs91U/htAGqqc3r27FkRgPjzzz+b+xw6dEgEIJ4/f75RNfMSmBUYDAYkJSVhxIgRFu0jRoxAYmKiRFW1fDqdDgDg4eEBALh27RqysrIszrNKpcLgwYPN5zkpKQkVFRUWffz9/REeHs5/Fr/xpz/9CWPHjsWwYcMs2nmem8Y333yDqKgoPProo/D29kavXr3wySefmNfzPDed++67D7t27cLFixcBACdOnMDBgwcxZswYADzXzaGpzumhQ4eg1Wot3ubQv39/aLXaRp93SV+FYS9ycnJgNBprvZHex8en1pvrqX5EUcTcuXNx3333ITw8HADM57Ku83zjxg1zH6VSCXd391p9+M/iFxs3bsTx48dx9OjRWut4npvG1atXsXr1asydOxevvfYajhw5ghdffBEqlQqTJ0/meW5Cr776KnQ6HUJDQyGXy2E0GvHWW2/h8ccfB8D/TTeHpjqnWVlZ8Pb2rrV/b2/vRp93BiArEgTB4rMoirXaqH5mzpyJkydP1vmS23s5z/xn8Yu0tDTMnj0bO3bsgFqtvmM/nufGMZlMiIqKwttvvw0A6NWrF86cOYPVq1dj8uTJ5n48z423adMmfPHFF/j3v/+Nbt26ISUlBXPmzIG/vz+mTJli7sdz3fSa4pzW1b8pzjsvgVmBl5cX5HJ5rbSanZ1dKx3T75s1axa++eYb7NmzB+3atTO3+/r6AsBdz7Ovry8MBgPy8/Pv2MfeJSUlITs7G5GRkVAoFFAoFNi3bx9WrFgBhUJhPk88z43j5+eHsLAwi7auXbuaXwDN/z03nb/85S+YN28eHnvsMURERCAuLg4vvfQS4uPjAfBcN4emOqe+vr64detWrf3fvn270eedAcgKlEolIiMjkZCQYNGekJCAmJgYiapqeURRxMyZM7Flyxbs3r0bwcHBFuuDg4Ph6+trcZ4NBgP27dtnPs+RkZFwcHCw6JOZmYnTp0/zn0W1oUOH4tSpU0hJSTEvUVFRePLJJ5GSkoIOHTrwPDeBAQMG1HqMw8WLFxEYGAiA/3tuSiUlJZDJLP/cyeVy823wPNdNr6nOaXR0NHQ6HY4cOWLuc/jwYeh0usaf90ZNoaZ6q7kN/l//+pd49uxZcc6cOaKzs7N4/fp1qUtrMV544QVRq9WKe/fuFTMzM81LSUmJuc8777wjarVaccuWLeKpU6fExx9/vM7bLtu1ayfu3LlTPH78uPjAAw/Y9a2s9fHru8BEkee5KRw5ckRUKBTiW2+9JV66dEn88ssvRScnJ/GLL74w9+F5bhpTpkwR27Zta74NfsuWLaKXl5f4yiuvmPvwXDdcYWGhmJycLCYnJ4sAxGXLlonJycnmx7s01TkdNWqU2L17d/HQoUPioUOHxIiICN4G39KsXLlSDAwMFJVKpdi7d2/z7dtUPwDqXNauXWvuYzKZxDfeeEP09fUVVSqVOGjQIPHUqVMW+yktLRVnzpwpenh4iI6OjuK4cePE1NRUKx9Ny/LbAMTz3DS+/fZbMTw8XFSpVGJoaKi4Zs0ai/U8z01Dr9eLs2fPFtu3by+q1WqxQ4cO4oIFC8Ty8nJzH57rhtuzZ0+d/02eMmWKKIpNd05zc3PFJ598UnR1dRVdXV3FJ598UszPz290/YIoimLjxpCIiIiIWhbOASIiIiK7wwBEREREdocBiIiIiOwOAxARERHZHQYgIiIisjsMQERERGR3GICIiIjI7jAAERFVCwoKwvLly6Uug4isgAGIiCTx9NNPY8KECQCAIUOGYM6cOVb77nXr1sHNza1W+9GjRzFt2jSr1UFE0lFIXQARUVMxGAxQKpX3vH2bNm2asBoismUcASIiST399NPYt28f/vGPf0AQBAiCgOvXrwMAzp49izFjxsDFxQU+Pj6Ii4tDTk6OedshQ4Zg5syZmDt3Lry8vDB8+HAAwLJlyxAREQFnZ2cEBARgxowZKCoqAgDs3bsXzzzzDHQ6nfn7Fi1aBKD2JbDU1FSMHz8eLi4u0Gg0mDRpEm7dumVev2jRIvTs2RPr169HUFAQtFotHnvsMRQWFpr7/Pe//0VERAQcHR3h6emJYcOGobi4uJnOJhHVFwMQEUnqH//4B6Kjo/Hcc88hMzMTmZmZCAgIQGZmJgYPHoyePXvi2LFj+PHHH3Hr1i1MmjTJYvvPPvsMCoUCP/30Ez7++GMAgEwmw4oVK3D69Gl89tln2L17N1555RUAQExMDJYvXw6NRmP+vpdffrlWXaIoYsKECcjLy8O+ffuQkJCAK1euIDY21qLflStXsG3bNnz33Xf47rvvsG/fPrzzzjsAgMzMTDz++OOYOnUqzp07h71792LixIngKxiJpMdLYEQkKa1WC6VSCScnJ/j6+prbV69ejd69e+Ptt982t3366acICAjAxYsX0aVLFwBAp06d8O6771rs89fziYKDg/G3v/0NL7zwAlatWgWlUgmtVgtBECy+77d27tyJkydP4tq1awgICAAArF+/Ht26dcPRo0fRp08fAIDJZMK6devg6uoKAIiLi8OuXbvw1ltvITMzE5WVlZg4cSICAwMBABEREY04W0TUVDgCREQ2KSkpCXv27IGLi4t5CQ0NBVA16lIjKiqq1rZ79uzB8OHD0bZtW7i6umLy5MnIzc1t0KWnc+fOISAgwBx+ACAsLAxubm44d+6cuS0oKMgcfgDAz88P2dnZAIAePXpg6NChiIiIwKOPPopPPvkE+fn59T8JRNRsGICIyCaZTCY8+OCDSElJsVguXbqEQYMGmfs5OztbbHfjxg2MGTMG4eHh2Lx5M5KSkrBy5UoAQEVFRb2/XxRFCILwu+0ODg4W6wVBgMlkAgDI5XIkJCTghx9+QFhYGD744AOEhITg2rVr9a6DiJoHAxARSU6pVMJoNFq09e7dG2fOnEFQUBA6depksfw29PzasWPHUFlZib///e/o378/unTpgoyMjN/9vt8KCwtDamoq0tLSzG1nz56FTqdD165d631sgiBgwIABWLx4MZKTk6FUKrF169Z6b09EzYMBiIgkFxQUhMOHD+P69evIycmByWTCn/70J+Tl5eHxxx/HkSNHcPXqVezYsQNTp069a3jp2LEjKisr8cEHH+Dq1atYv349Pvroo1rfV1RUhF27diEnJwclJSW19jNs2DB0794dTz75JI4fP44jR45g8uTJGDx4cJ2X3epy+PBhvP322zh27BhSU1OxZcsW3L59u0EBioiaBwMQEUnu5ZdfhlwuR1hYGNq0aYPU1FT4+/vjp59+gtFoxMiRIxEeHo7Zs2dDq9VCJrvzf7p69uyJZcuWYenSpQgPD8eXX36J+Ph4iz4xMTGYPn06YmNj0aZNm1qTqIGqkZtt27bB3d0dgwYNwrBhw9ChQwds2rSp3sel0Wiwf/9+jBkzBl26dMHChQvx97//HaNHj67/ySGiZiGIvB+TiIiI7AxHgIiIiMjuMAARERGR3WEAIiIiIrvDAERERER2hwGIiIiI7A4DEBEREdkdBiAiIiKyOwxAREREZHcYgIiIiMjuMAARERGR3WEAIiIiIrvDAERERER25/8BGS9irfyi4KwAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "IndexError",
     "evalue": "index 2 is out of bounds for axis 0 with size 2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_30208\\2532620725.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[1;31m# 注意：这里的theta[2]对应的是X2的系数，因为X_b包含了偏置项，所以theta[0]是偏置项\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m \u001b[0mplot_decision_boundary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_b\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtheta_opt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_30208\\2532620725.py\u001b[0m in \u001b[0;36mplot_decision_boundary\u001b[1;34m(X, y, theta)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[0mx_vals\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m     \u001b[0my_vals\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtheta\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mtheta\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mx_vals\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mtheta\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Decision Boundary'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: index 2 is out of bounds for axis 0 with size 2"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiMAAAGdCAYAAADAAnMpAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/d3fzzAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAngklEQVR4nO3df1Bd9Z3/8dflAhdjwrUmBiFgZFObxKbbFhITiFh/4qbWaeY7s4njLDFunK+s2phEsclmplWnM7SdrW20wvojmul31GaVxDqzWSszDQkxsdMwZNcVt3YTFZJAKen2gnELAT7fP+i9crm/zrlc+HDh+Zi5M3L4nHPe7/M5J/fl5XDwGGOMAAAALMmwXQAAAJjZCCMAAMAqwggAALCKMAIAAKwijAAAAKsIIwAAwCrCCAAAsIowAgAArMq0XYATw8PDOnv2rObMmSOPx2O7HAAA4IAxRn19fSooKFBGRuzPP9IijJw9e1ZFRUW2ywAAAEno6OhQYWFhzO+nRRiZM2eOpJFmcnNzLVcDAACc6O3tVVFRUeh9PJa0CCPBH83k5uYSRgAASDOJbrHgBlYAAGAVYQQAAFhFGAEAAFYRRgAAgFWEEQAAYBVhBAAAWEUYAQAAVhFGAACAVWnx0LOZbmhgSO/WNevTk52atShfX7qvQt5sr+2yQqZ6fakSq8+BTwZ0rKpOOnlSWrRIZf/vPmXPzrZdriOxehrPnE7k+TA0MKQTu5rU+4smSVLuN6/XVx68Pmz7wf2f/+CMBjv/IO/ll2n24gVhdSRb49j1rr6nXG3PH9WnJzvlu2K+JOnPH3ZF7FdSzP25qcXpWDfbHH3+Dl9ZrDllX9LA2Z6I9QY+GdDbdz6lnN8c0WDObOX83yqVPHRTwuMWrZahgaG/bKtZWf3n1bt4ufz/52Z95cHrHR+r4PHub++OmIux53KicyZR7fHOpVi9jq0v0ZzH6zvWsbz6nnK9W98cs7dQ76//SlmdHbqQX6TctTdGjJkS/34blw4dOmS+8Y1vmPz8fCPJ7N+/P+E6TU1NpqSkxPh8PlNcXGzq6+td7TMQCBhJJhAIuC037R2raTBnvIXGSKHXGW+hOVbTYLs0Y8zUry9VYvaZ901zQd6w5RfkNQdX1NguOaFYPR1cUZP0nE7k+XCspsH0eOaGbdtIpsczN7T9aPsfW0eyNUZbb+zcR3v1eOZG1J1MLU7HutnmwRU1cXsIrndwRY0ZVEbE9wOaHfe4RaulV7PNkDxR99er2a6OVby5CJ7Lic4ZN7W7Pe5O+4h3jsTbfrQ5CfYW63oZO2ai//12+v7tOowcOHDA7Ny50zQ0NDgKI6dOnTKzZs0yDz74oGlrazPPPfecycrKMq+99prjfc7UMHKspsEMyWOGxpxII8s81t/wp3p9qRK7T5nhv7xGLw8um8qBJFFPyczpRJ4Px2oaoh5rM6regytqzJA8UccEe0u2v1i9xdpXtPMhcn/Oa3F6bN3MwcEVNTGP6ej1hmP0MPrYxwpPsY5ZrH26PVbx5iLW9Zmo7tG1xz+XEh/3aMcyeB4669sTCg1Oz7/gPhId5/Fc625MWBgJW1mJw8gjjzxilixZErbs3nvvNatWrXK8n5kYRgb7B80Zb2HMk3tIHnPaW2QG+wepbwIl6jPexX5BXtPf12+7hQjJ9hRvTifyfBjsHzRnMgrjvmmOHO+MuG8Gid4IY9WYqLdkX05rcXps+/v6Hc9Bf1+/uSBv0mFq7PdPexaEHbdUH7NENcRbL27dGYVJz/eQ5Oi4j6ePIXnMGW+hOeNZ4OpYOtlPMtdCMpy+f0/4DazHjh1TZWVl2LJbb71Vx48f14ULF6Ku09/fr97e3rDXTPNuXbMKhk7HvMM4Q0YLhjr0bl3zpNYVNNXrS5VEfcb6008eSZkaGvlZ/BSTbE/x5nQiz4d365pVMHw6Zl1S8HgPJ7wj3yP3/SXqLVlOa3F6bI9V1Tmeg2NVdcrUUNxj6qTO4PcXmDNhxy3VxyxRDfHWi/e9BcOnk57vDMnRcR+7Tzd9ZMioYOi0CswZV8fSyX6SuRYm0oSHka6uLuXl5YUty8vL0+DgoHp6eqKuU1tbK7/fH3oVFRVNdJlTzqcnO1M6LtWmen2pMu76T55MTSEpNN6eoq0/kefDZJ9DY/dn8xz+9GSn8/07PNc+Pdk5Iefl6DrT6bof93xPwWs8VSZzHiflV3vH/ulgY0zU5UE7duxQIBAIvTo6Oia8xqlm1qL8lI5LtaleX6qMu/5Fi1JTSAqNt6do60/k+TDZ59DY/dk8h2ctyne+f4fn2qxF+RNyXo6uM52u+3HP9xS8xlNlUudxPD8LkhLfM1JRUWE2b94ctmzfvn0mMzPTDAwMONrPzL5nJPpd57bvyZjq9aVKoj7T+54Rdz05u2ck9eeDu3tGou9/PD8nT9Rbsi/394zEP7af3buQeA4m756R1BwzO/eMxK898p6RxL2O754R58eSe0aiKCsrU2NjY9iyt956S8uXL1dWVtZE7z5tebO9at+2S5I0POYne8GvO7b9xNrzPKZ6fakSv88RZsw6wa+PrNg2JZ834qSn4THrJJrTiTwfvNletT80su2xxzq4zEg6suKhqLV/Vsdn67upMV5v0eqJVd/Y/TmtxemxzZ6d7XgOsmdn68iKbQl7GL2deMe+4+Enw45bomMWa59uj9XYdcPXc1D3Q7sSzHd0I8s9CY97+Dqe0L6dHJPgmPZtu9T+8JNRtx+vt1jfHztmyvz77Tbl9PX1mdbWVtPa2mokmSeeeMK0traajz/+2BhjzPbt201VVVVofPBXe7du3Wra2trM7t27+dVeF6L9Hvhpb9GU+bXZqV5fqsTsc5o9Z+S0tyjqc0aczulEng+xnpvwB4fPGQnWkWyNyT5n5A9RniGRTC1Ox7rZZqLnjATXS+VzRgJxnjMSiPKckXjHKt5cBM/lROeMm9rdHnenfcQ7R+JtP9qc/MHBc0b+EOc5I6n+99vp+7fHGBMrPEXV1NSkG264IWL5XXfdpT179mjjxo366KOP1NTUFPreoUOHtHXrVr333nsqKCjQt7/9bVVXVzveZ29vr/x+vwKBgHJzc92UOy1MmSfkxTDV60sVnsDKE1h5AitPYOUJrO44ff92HUZsmOlhBACAdOT0/Zs/lAcAAKwijAAAAKsIIwAAwCrCCAAAsIowAgAArCKMAAAAqwgjAADAKsIIAACwijACAACsIowAAACrCCMAAMAqwggAALCKMAIAAKwijAAAAKsIIwAAwCrCCAAAsIowAgAArCKMAAAAqwgjAADAKsIIAACwijACAACsIowAAACrCCMAAMAqwggAALCKMAIAAKwijAAAAKsIIwAAwCrCCAAAsIowAgAArCKMAAAAqwgjAADAKsIIAACwijACAACsIowAAACrCCMAAMAqwggAALCKMAIAAKwijAAAAKsIIwAAwCrCCAAAsIowAgAArCKMAAAAqwgjAADAKsIIAACwijACAACsIowAAACrCCMAAMAqwggAALCKMAIAAKwijAAAAKsIIwAAwCrCCAAAsIowAgAArCKMAAAAqwgjAADAKsIIAACwijACAACsSiqM1NXVqbi4WDk5OSotLVVzc3Pc8S+99JK+/OUva9asWcrPz9fdd9+tc+fOJVUwAACYXlyHkb1792rLli3auXOnWltbVVFRoTVr1qi9vT3q+CNHjmjDhg3atGmT3nvvPb366qv6zW9+o3vuuWfcxQMAgPTnOow88cQT2rRpk+655x4tXbpUP/nJT1RUVKT6+vqo49955x1deeWV2rx5s4qLi3Xttdfq3nvv1fHjx8ddPAAASH+uwsjAwIBaWlpUWVkZtryyslJHjx6Nuk55eblOnz6tAwcOyBij3//+93rttdd02223xdxPf3+/ent7w14AAGB6chVGenp6NDQ0pLy8vLDleXl56urqirpOeXm5XnrpJa1fv17Z2dm6/PLLdckll+ipp56KuZ/a2lr5/f7Qq6ioyE2ZAAAgjSR1A6vH4wn72hgTsSyora1Nmzdv1ne+8x21tLTozTff1Icffqjq6uqY29+xY4cCgUDo1dHRkUyZAAAgDWS6GTxv3jx5vd6IT0G6u7sjPi0Jqq2t1erVq1VTUyNJ+uu//mtdfPHFqqio0Pe+9z3l5+dHrOPz+eTz+dyUBgAA0pSrT0ays7NVWlqqxsbGsOWNjY0qLy+Pus6nn36qjIzw3Xi9Xkkjn6gAAICZzfWPabZt26bnn39eL7zwgt5//31t3bpV7e3toR+77NixQxs2bAiNv/3227Vv3z7V19fr1KlTevvtt7V582Zdc801KigoSF0nAAAgLbn6MY0krV+/XufOndPjjz+uzs5OLVu2TAcOHNDChQslSZ2dnWHPHNm4caP6+vr005/+VA899JAuueQS3XjjjfrBD36Qui4AAEDa8pg0+FlJb2+v/H6/AoGAcnNzbZcDAAAccPr+zd+mAQAAVhFGAACAVYQRAABgFWEEAABYRRgBAABWEUYAAIBVhBEAAGAVYQQAAFhFGAEAAFYRRgAAgFWEEQAAYBVhBAAAWEUYAQAAVhFGAACAVYQRAABgFWEEAABYRRgBAABWEUYAAIBVhBEAAGAVYQQAAFhFGAEAAFYRRgAAgFWEEQAAYBVhBAAAWEUYAQAAVhFGAACAVYQRAABgFWEEAABYRRgBAABWEUYAAIBVhBEAAGAVYQQAAFhFGAEAAFYRRgAAgFWEEQAAYBVhBAAAWEUYAQAAVhFGAACAVYQRAABgFWEEAABYRRgBAABWEUYAAIBVhBEAAGAVYQQAAFhFGAEAAFYRRgAAgFWEEQAAYBVhBAAAWEUYAQAAVhFGAACAVYQRAABgFWEEAABYRRgBAABWEUYAAIBVhBEAAGAVYQQAAFiVVBipq6tTcXGxcnJyVFpaqubm5rjj+/v7tXPnTi1cuFA+n0+LFi3SCy+8kFTBAABgesl0u8LevXu1ZcsW1dXVafXq1XrmmWe0Zs0atbW16Yorroi6zrp16/T73/9eu3fv1uc//3l1d3drcHBw3MUDAID05zHGGDcrrFy5UiUlJaqvrw8tW7p0qdauXava2tqI8W+++abuuOMOnTp1SpdeemlSRfb29srv9ysQCCg3NzepbQAAgMnl9P3b1Y9pBgYG1NLSosrKyrDllZWVOnr0aNR13njjDS1fvlw//OEPtWDBAn3hC1/Qww8/rP/93/+NuZ/+/n719vaGvQAAwPTk6sc0PT09GhoaUl5eXtjyvLw8dXV1RV3n1KlTOnLkiHJycrR//3719PTovvvu0x//+MeY943U1tbqsccec1MaAABIU0ndwOrxeMK+NsZELAsaHh6Wx+PRSy+9pGuuuUZf//rX9cQTT2jPnj0xPx3ZsWOHAoFA6NXR0ZFMmQAAIA24+mRk3rx58nq9EZ+CdHd3R3xaEpSfn68FCxbI7/eHli1dulTGGJ0+fVpXXXVVxDo+n08+n89NaQAAIE25+mQkOztbpaWlamxsDFve2Nio8vLyqOusXr1aZ8+e1SeffBJa9sEHHygjI0OFhYVJlAwAAKYT1z+m2bZtm55//nm98MILev/997V161a1t7erurpa0siPWDZs2BAaf+edd2ru3Lm6++671dbWpsOHD6umpkZ///d/r4suuih1nQAAgLTk+jkj69ev17lz5/T444+rs7NTy5Yt04EDB7Rw4UJJUmdnp9rb20PjZ8+ercbGRn3rW9/S8uXLNXfuXK1bt07f+973UtcFAABIW66fM2IDzxkBACD9TMhzRgAAAFKNMAIAAKwijAAAAKsIIwAAwCrCCAAAsIowAgAArCKMAAAAqwgjAADAKsIIAACwijACAACsIowAAACrCCMAAMAqwggAALCKMAIAAKwijAAAAKsIIwAAwCrCCAAAsIowAgAArCKMAAAAqwgjAADAKsIIAACwijACAACsIowAAACrCCMAAMAqwggAALCKMAIAAKwijAAAAKsIIwAAwCrCCAAAsIowAgAArCKMAAAAqwgjAADAKsIIAACwijACAACsIowAAACrCCMAAMAqwggAALCKMAIAAKwijAAAAKsIIwAAwCrCCAAAsIowAgAArCKMAAAAqwgjAADAKsIIAACwijACAACsIowAAACrCCMAAMAqwggAALCKMAIAAKwijAAAAKsIIwAAwCrCCAAAsIowAgAArCKMAAAAqwgjAADAqqTCSF1dnYqLi5WTk6PS0lI1Nzc7Wu/tt99WZmamvvKVrySzWwAAMA25DiN79+7Vli1btHPnTrW2tqqiokJr1qxRe3t73PUCgYA2bNigm266KeliAQDA9OMxxhg3K6xcuVIlJSWqr68PLVu6dKnWrl2r2tramOvdcccduuqqq+T1evX666/rxIkTjvfZ29srv9+vQCCg3NxcN+UCAABLnL5/u/pkZGBgQC0tLaqsrAxbXllZqaNHj8Zc78UXX9TJkyf13e9+19F++vv71dvbG/YCAADTk6sw0tPTo6GhIeXl5YUtz8vLU1dXV9R1fve732n79u166aWXlJmZ6Wg/tbW18vv9oVdRUZGbMgEAQBpJ6gZWj8cT9rUxJmKZJA0NDenOO+/UY489pi984QuOt79jxw4FAoHQq6OjI5kyAQBAGnD2UcVfzJs3T16vN+JTkO7u7ohPSySpr69Px48fV2trqx544AFJ0vDwsIwxyszM1FtvvaUbb7wxYj2fzyefz+emNAAAkKZcfTKSnZ2t0tJSNTY2hi1vbGxUeXl5xPjc3Fy9++67OnHiROhVXV2txYsX68SJE1q5cuX4qgcAAGnP1ScjkrRt2zZVVVVp+fLlKisr07PPPqv29nZVV1dLGvkRy5kzZ/Szn/1MGRkZWrZsWdj68+fPV05OTsRyAAAwM7kOI+vXr9e5c+f0+OOPq7OzU8uWLdOBAwe0cOFCSVJnZ2fCZ44AAAAEuX7OiA08ZwQAgPQzIc8ZAQAASDXCCAAAsIowAgAArCKMAAAAqwgjAADAKsIIAACwijACAACsIowAAACrCCMAAMAqwggAALCKMAIAAKwijAAAAKsIIwAAwCrCCAAAsIowAgAArCKMAAAAqwgjAADAKsIIAACwijACAACsIowAAACrCCMAAMAqwggAALCKMAIAAKwijAAAAKsIIwAAwCrCCAAAsIowAgAArCKMAAAAqwgjAADAKsIIAACwijACAACsIowAAACrCCMAAMAqwggAALCKMAIAAKwijAAAAKsIIwAAwCrCCAAAsIowAgAArCKMAAAAqwgjAADAKsIIAACwijACAACsIowAAACrCCMAAMAqwggAALCKMAIAAKwijAAAAKsIIwAAwCrCCAAAsIowAgAArCKMAAAAqwgjAADAKsIIAACwijACAACsIowAAACrkgojdXV1Ki4uVk5OjkpLS9Xc3Bxz7L59+3TLLbfosssuU25ursrKyvTLX/4y6YIBAMD04jqM7N27V1u2bNHOnTvV2tqqiooKrVmzRu3t7VHHHz58WLfccosOHDiglpYW3XDDDbr99tvV2to67uIBAED68xhjjJsVVq5cqZKSEtXX14eWLV26VGvXrlVtba2jbXzxi1/U+vXr9Z3vfMfR+N7eXvn9fgUCAeXm5ropFwAAWOL0/dvVJyMDAwNqaWlRZWVl2PLKykodPXrU0TaGh4fV19enSy+9NOaY/v5+9fb2hr0AAMD05CqM9PT0aGhoSHl5eWHL8/Ly1NXV5WgbP/rRj3T+/HmtW7cu5pja2lr5/f7Qq6ioyE2ZAAAgjSR1A6vH4wn72hgTsSyaV155RY8++qj27t2r+fPnxxy3Y8cOBQKB0KujoyOZMgEAQBrIdDN43rx58nq9EZ+CdHd3R3xaMtbevXu1adMmvfrqq7r55pvjjvX5fPL5fG5KAwAAacrVJyPZ2dkqLS1VY2Nj2PLGxkaVl5fHXO+VV17Rxo0b9fLLL+u2225LrlIAADAtufpkRJK2bdumqqoqLV++XGVlZXr22WfV3t6u6upqSSM/Yjlz5ox+9rOfSRoJIhs2bNCuXbu0atWq0KcqF110kfx+fwpbAQAA6ch1GFm/fr3OnTunxx9/XJ2dnVq2bJkOHDighQsXSpI6OzvDnjnyzDPPaHBwUPfff7/uv//+0PK77rpLe/bsGX8HAAAgrbl+zogNPGcEAID0MyHPGQEAAEg1wggAALCKMAIAAKwijAAAAKsIIwAAwCrCCAAAsIowAgAArCKMAAAAqwgjAADAKsIIAACwijACAACsIowAAACrCCMAAMAqwggAALCKMAIAAKwijAAAAKsIIwAAwCrCCAAAsIowAgAArCKMAAAAqwgjAADAKsIIAACwijACAACsIowAAACrCCMAAMAqwggAALCKMAIAAKwijAAAAKsIIwAAwCrCCAAAsIowAgAArCKMAAAAqwgjAADAKsIIAACwijACAACsIowAAACrCCMAAMAqwggAALCKMAIAAKwijAAAAKsIIwAAwCrCCAAAsIowAgAArCKMAAAAqwgjAADAKsIIAACwijACAACsIowAAACrCCMAAMAqwggAALCKMAIAAKwijAAAAKsIIwAAwCrCCAAAsIowAgAArMq0XYAtQ0NSc7PU2Snl50sVFZLXG/m9+fNHlnV3j4wrL5eOHg1fb2hIqquTTp6UFi2S7rtPys7+bDtnzkhdXdIf/yhlZEjXXz+ynWeeiVwnUW3Rvh+sKdp+rr8+ct2mppGXFDlmYCB+L6NrkmJvK7ifX/1K6uiQioqkG28c+b4Uv79Uz2ms4+VkDkcfj+Ji6Utfknp6YtedaO6i1Tz6GF533cjcxTvfovU03mPoZnup2nescy3WvsZei6k6b5LlpH4ADpkkPP300+bKK680Pp/PlJSUmMOHD8cd39TUZEpKSozP5zPFxcWmvr7e1f4CgYCRZAKBQDLlRmhoMKaw0Bjps1dh4cjyaN8b/fJ6w7+ePduYjIzIMd/8ZvztRNtuTU382mLVPram0a+5c8PXnTs39piamshtxepl7tyR3qNtq6Ym+n6Cx2vs90b3l+o5ralJfLxizeGKFfGP7di6E81dtJpjHadYtcbqaTzH0E3dbnuMJda5VlMTf1+pPm+S5aR+AM7fv12HkZ///OcmKyvLPPfcc6atrc08+OCD5uKLLzYff/xx1PGnTp0ys2bNMg8++KBpa2szzz33nMnKyjKvvfaa432mMow0NBjj8UT+wxZt2VR5eTwjr5qa5OusqbHfR6L+xvNmamP+Rtcd77yK1ltDw9Q4hm7qdttjLInOxWAoTzSn4z1vkuWkfgAjJiyMXHPNNaa6ujps2ZIlS8z27dujjn/kkUfMkiVLwpbde++9ZtWqVY73maowMjjo7tOKqfaK93/pTt6sbNefqL6iopE5Sqc59XhG9r9ggfPeBgfjj5+sY5jo2I3enpux8fT3Jz6PvV5jCgom9rxJltP6+/snpx5gqnP6/u3qBtaBgQG1tLSosrIybHllZaWOHj0adZ1jx45FjL/11lt1/PhxXbhwIeo6/f396u3tDXulQnOzdPp0SjZlxdBQ8usak7o6JoIxI/eWNDe7W8/2nBozsv8zZ+KPGd1b8D6iiajFzTFMdOxGb8/N2Hjq6hKfx0ND0tmz8ce43W+qOK2/rm5y6gGmC1dhpKenR0NDQ8rLywtbnpeXp66urqjrdHV1RR0/ODionp6eqOvU1tbK7/eHXkVFRW7KjKmzMyWbwQRyO0fpNKfBWie6ZqfbdzMuVds8edLZdtyarPPAaf0T1ScwXSX1q70ejyfsa2NMxLJE46MtD9qxY4cCgUDo1dHRkUyZEfLzU7IZTCC3c5ROcxqsdaJrdrp9N+NStc1Fi5xtx63JOg+c1j9RfQLTlaswMm/ePHm93ohPQbq7uyM+/Qi6/PLLo47PzMzU3Llzo67j8/mUm5sb9kqFigqpsFCKk5umNK83+dqnes8ez8iv/wZ/Zdgp23Pq8Yzsf8GC2DWM7a2iYmT8RNTi5hgmOnajt+dmbDz33Zf413G9XqmgwNmcJnveJMtp/ffdNzn1ANOFqzCSnZ2t0tJSNTY2hi1vbGxUeXl51HXKysoixr/11ltavny5srKyXJY7Pl6vtGvXyH+P/Ydu9Ne237ij1ebxSNu2Rf++Ew8/PP66Jkqwn5/8xP1zI+LN6UQL7m/XLunJJ6PXEK03r/ez8amuxc0xdHI9BLfnZmw82dmfncexbNsmPfVU9H0lu99UcVo/zxsBXHJ7Z2zwV3t3795t2trazJYtW8zFF19sPvroI2OMMdu3bzdVVVWh8cFf7d26datpa2szu3fvtvqrvcZEf35BUdHUfc5IsLZYtaf7c0ZG95fqOXXynJE5c5J7zsjYuhPNXbSa3T5nJFZP4zmGbup222MsqXjOSCrOm2TxnBHAGafv3x5j3P+eRV1dnX74wx+qs7NTy5Yt049//GNdd911kqSNGzfqo48+UlPwkZKSDh06pK1bt+q9995TQUGBvv3tb6u6utrx/np7e+X3+xUIBFL2IxuewDryNU9g5QmsbuvmCawjeAIrkJjT9++kwshkm4gwAgAAJpbT92/+UB4AALCKMAIAAKwijAAAAKsIIwAAwCrCCAAAsIowAgAArCKMAAAAqwgjAADAKsIIAACwKtN2AU4EHxLb29truRIAAOBU8H070cPe0yKM9PX1SZKKioosVwIAANzq6+uT3++P+f20+Ns0w8PDOnv2rObMmSNPCv5OfG9vr4qKitTR0TFt/9bNTOhRmhl9zoQepZnRJz1OHzOhz1T0aIxRX1+fCgoKlJER+86QtPhkJCMjQ4WFhSnfbm5u7rQ9iYJmQo/SzOhzJvQozYw+6XH6mAl9jrfHeJ+IBHEDKwAAsIowAgAArJqRYcTn8+m73/2ufD6f7VImzEzoUZoZfc6EHqWZ0Sc9Th8zoc/J7DEtbmAFAADT14z8ZAQAAEwdhBEAAGAVYQQAAFhFGAEAAFZNizBSV1en4uJi5eTkqLS0VM3NzXHHHzp0SKWlpcrJydFf/dVf6Z//+Z8jxjQ0NOjqq6+Wz+fT1Vdfrf37909U+Y656XPfvn265ZZbdNlllyk3N1dlZWX65S9/GTZmz5498ng8Ea8///nPE91KTG56bGpqilr/f/3Xf4WNS/e53LhxY9Q+v/jFL4bGTLW5PHz4sG6//XYVFBTI4/Ho9ddfT7hOul2XbntM12vSbZ/peF267TEdr8na2lqtWLFCc+bM0fz587V27Vr99re/TbjeZF2XaR9G9u7dqy1btmjnzp1qbW1VRUWF1qxZo/b29qjjP/zwQ339619XRUWFWltb9Y//+I/avHmzGhoaQmOOHTum9evXq6qqSv/+7/+uqqoqrVu3Tr/+9a8nq60Ibvs8fPiwbrnlFh04cEAtLS264YYbdPvtt6u1tTVsXG5urjo7O8NeOTk5k9FSBLc9Bv32t78Nq/+qq64KfW86zOWuXbvC+uvo6NCll16qv/3bvw0bN5Xm8vz58/ryl7+sn/70p47Gp+N16bbHdLwmJfd9BqXTdem2x3S8Jg8dOqT7779f77zzjhobGzU4OKjKykqdP38+5jqTel2aNHfNNdeY6urqsGVLliwx27dvjzr+kUceMUuWLAlbdu+995pVq1aFvl63bp35m7/5m7Axt956q7njjjtSVLV7bvuM5uqrrzaPPfZY6OsXX3zR+P3+VJU4bm57PHjwoJFk/ud//ifmNqfjXO7fv994PB7z0UcfhZZNtbkcTZLZv39/3DHpel0GOekxmql+TY7lpM90vS6DkpnLdLsmjTGmu7vbSDKHDh2KOWYyr8u0/mRkYGBALS0tqqysDFteWVmpo0ePRl3n2LFjEeNvvfVWHT9+XBcuXIg7JtY2J1oyfY41PDysvr4+XXrppWHLP/nkEy1cuFCFhYX6xje+EfF/aZNlPD1+9atfVX5+vm666SYdPHgw7HvTcS53796tm2++WQsXLgxbPlXmMhnpeF2O11S/Jscrna7L8UrHazIQCEhSxPk32mRel2kdRnp6ejQ0NKS8vLyw5Xl5eerq6oq6TldXV9Txg4OD6unpiTsm1jYnWjJ9jvWjH/1I58+f17p160LLlixZoj179uiNN97QK6+8opycHK1evVq/+93vUlq/E8n0mJ+fr2effVYNDQ3at2+fFi9erJtuukmHDx8OjZluc9nZ2al/+7d/0z333BO2fCrNZTLS8bocr6l+TSYrHa/L8UjHa9IYo23btunaa6/VsmXLYo6bzOsyLf5qbyIejyfsa2NMxLJE48cud7vNyZBsTa+88ooeffRR/eIXv9D8+fNDy1etWqVVq1aFvl69erVKSkr01FNP6cknn0xd4S646XHx4sVavHhx6OuysjJ1dHTon/7pn3Tdddcltc3JkmxNe/bs0SWXXKK1a9eGLZ+Kc+lWul6XyUina9KtdL4uk5GO1+QDDzyg//iP/9CRI0cSjp2s6zKtPxmZN2+evF5vRALr7u6OSGpBl19+edTxmZmZmjt3btwxsbY50ZLpM2jv3r3atGmT/uVf/kU333xz3LEZGRlasWKFleQ+nh5HW7VqVVj902kujTF64YUXVFVVpezs7Lhjbc5lMtLxukxWulyTqTTVr8tkpeM1+a1vfUtvvPGGDh48qMLCwrhjJ/O6TOswkp2drdLSUjU2NoYtb2xsVHl5edR1ysrKIsa/9dZbWr58ubKysuKOibXNiZZMn9LI/31t3LhRL7/8sm677baE+zHG6MSJE8rPzx93zW4l2+NYra2tYfVPl7mURu6G/+///m9t2rQp4X5szmUy0vG6TEY6XZOpNNWvy2Sl0zVpjNEDDzygffv26Ve/+pWKi4sTrjOp16Wr212noJ///OcmKyvL7N6927S1tZktW7aYiy++OHRX8/bt201VVVVo/KlTp8ysWbPM1q1bTVtbm9m9e7fJysoyr732WmjM22+/bbxer/n+979v3n//ffP973/fZGZmmnfeeWfS+wty2+fLL79sMjMzzdNPP206OztDrz/96U+hMY8++qh58803zcmTJ01ra6u5++67TWZmpvn1r3896f0Z477HH//4x2b//v3mgw8+MP/5n/9ptm/fbiSZhoaG0JjpMJdBf/d3f2dWrlwZdZtTbS77+vpMa2uraW1tNZLME088YVpbW83HH39sjJke16XbHtPxmjTGfZ/peF267TEona7Jf/iHfzB+v980NTWFnX+ffvppaIzN6zLtw4gxxjz99NNm4cKFJjs725SUlIT9qtJdd91lvva1r4WNb2pqMl/96ldNdna2ufLKK019fX3ENl999VWzePFik5WVZZYsWRJ2Idnips+vfe1rRlLE66677gqN2bJli7niiitMdna2ueyyy0xlZaU5evToJHYUyU2PP/jBD8yiRYtMTk6O+dznPmeuvfZa86//+q8R20z3uTTGmD/96U/moosuMs8++2zU7U21uQz+emes8286XJdue0zXa9Jtn+l4XSZzvqbbNRmtP0nmxRdfDI2xeV16/lIkAACAFWl9zwgAAEh/hBEAAGAVYQQAAFhFGAEAAFYRRgAAgFWEEQAAYBVhBAAAWEUYAQAAVhFGAACAVYQRAABgFWEEAABYRRgBAABW/X8WcuUlgPOENAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np  \n",
    "import matplotlib.pyplot as plt\n",
    "def sigmoid(z):  \n",
    "    return 1 / (1 + np.exp(-z))\n",
    "def predict_prob(X, theta):  \n",
    "    return sigmoid(np.dot(X, theta))\n",
    "def compute_loss(X, y, theta):  \n",
    "    m = len(y)  \n",
    "    h = predict_prob(X, theta)  \n",
    "    epsilon = 1e-15  # 防止log(0)  \n",
    "    h_clipped = np.clip(h, epsilon, 1 - epsilon)  \n",
    "    log_likelihood = -np.sum(y * np.log(h_clipped) + (1 - y) * np.log(1 - h_clipped))  \n",
    "    return log_likelihood / m\n",
    "\n",
    "def compute_gradient(X, y, theta):  \n",
    "    m = len(y)  \n",
    "    h = predict_prob(X, theta)  \n",
    "    gradient = (1 / m) * np.dot(X.T, (h - y))  \n",
    "    return gradient\n",
    "def gradient_descent(X, y, theta, learning_rate, num_iterations):  \n",
    "    m = len(y)  #样本数\n",
    "    cost_history = np.zeros(num_iterations)  \n",
    "      \n",
    "    for i in range(num_iterations):  \n",
    "        gradient = compute_gradient(X, y, theta)  \n",
    "        theta = theta - learning_rate * gradient  #梯度下降\n",
    "        cost_history[i] = compute_loss(X, y, theta)  \n",
    "          \n",
    "    return theta, cost_history\n",
    "# 生成数据  \n",
    "np.random.seed(42)  \n",
    "X = 2 * np.random.rand(100, 1)  \n",
    "y = 4 + 3 * X + np.random.randn(100, 1)  \n",
    "y = (y > 6.5).astype(int)  # 简单的二分类  \n",
    "  \n",
    "# 添加偏置项b  \n",
    "X_b = np.c_[np.ones((100, 1)), X]  # 添加一列1作为偏置项\n",
    "# 初始化参数  \n",
    "theta = np.random.randn(2, 1)  \n",
    "learning_rate = 0.1  \n",
    "num_iterations = 1000  \n",
    "  \n",
    "# 训练模型  \n",
    "theta_opt, cost_history = gradient_descent(X_b, y, theta, learning_rate, num_iterations)\n",
    "# 绘制损失函数的变化  \n",
    "plt.plot(range(num_iterations), cost_history)  \n",
    "plt.xlabel('Iterations')  \n",
    "plt.ylabel('Cost')  \n",
    "plt.title('Cost over time')  \n",
    "plt.show()  \n",
    "  \n",
    "# 绘制决策边界  \n",
    "def plot_decision_boundary(X, y, theta):  \n",
    "    plt.scatter(X[:, 1], y, color='blue', label='y=0')  \n",
    "    plt.scatter(X[y==1, 1], y[y==1], color='red', label='y=1')  \n",
    "      \n",
    "    x_vals = np.array([-2, 2])  \n",
    "    y_vals = -(theta[0] + theta[1] * x_vals) / theta[2]  \n",
    "    plt.plot(x_vals, y_vals, label='Decision Boundary')  \n",
    "      \n",
    "    plt.xlabel('X1')  \n",
    "    plt.ylabel('y')  \n",
    "    plt.legend()  \n",
    "    plt.show()  \n",
    "  \n",
    "# 注意：这里的theta[2]对应的是X2的系数，因为X_b包含了偏置项，所以theta[0]是偏置项  \n",
    "plot_decision_boundary(X_b, y.flatten(), theta_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  \n",
    "class LogisticRegression:  \n",
    "    def __init__(self,X,y, learning_rate=0.01, num_iterations=1000, solver='gd', tol=1e-4, C=1.0, verbose=False): \n",
    "        # print(y)\n",
    "        self.labels=len(y[1]) #标签数\n",
    "        # print(\"labels\",self.labels)\n",
    "        self.samples_num=X.shape[0] \n",
    "        self.features_num=X.shape[1]\n",
    "\n",
    "\n",
    "        ones=np.ones((X.shape[0],1))\n",
    "        self.dataX=np.c_[ones,X]\n",
    "        self.y=y\n",
    "        self.learning_rate=learning_rate\n",
    "        self.num_iterations=num_iterations\n",
    "        self.solver=solver\n",
    "        self.tol=tol\n",
    "        self.C=C\n",
    "        self.verbose=verbose\n",
    "        self.theta=np.zeros((self.dataX.shape[1], self.labels)) # 参数初始化\n",
    "        self.loss_history=[]\n",
    "        self.theta_history=[]\n",
    "        self.fit()\n",
    "    def softmax(self, z):\n",
    "        return np.exp(z) / np.sum(np.exp(z), axis=1, keepdims=True)\n",
    "    def fit(self):\n",
    "        if self.solver=='gd':\n",
    "            self.gradient_descent()\n",
    "        elif self.solver=='sgd':\n",
    "            self.stochastic_gradient_descent()\n",
    "        elif self.solver=='newton':\n",
    "            self.newton_method()\n",
    "    def gradient_descent(self):\n",
    "        for i in range(self.num_iterations):\n",
    "            # print(\"self.dataX.shape\",self.dataX.shape)\n",
    "            # print(\"self.theta.shape\",self.theta.shape)\n",
    "            z=np.dot(self.dataX,self.theta)\n",
    "            # print(z)\n",
    "            # print(\"z.shape\",z.shape)\n",
    "\n",
    "            y_pred=self.softmax(z)\n",
    "            # print(y_pred)\n",
    "            # print(\"self.y\",self.y)\n",
    "            loss=-np.sum(self.y*np.log(y_pred))/self.samples_num\n",
    "            self.loss_history.append(loss)\n",
    "            self.theta_history.append(self.theta)\n",
    "            gradient=np.dot(self.dataX.T,(y_pred-self.y))/self.samples_num\n",
    "            self.theta-=self.learning_rate*gradient\n",
    "            if self.verbose and i%100==0:\n",
    "                print('Iteration %d, Loss: %f'%(i,loss))\n",
    "    def predict(self,X):\n",
    "        ones=np.ones((X.shape[0],1))\n",
    "        dataX=np.c_[ones,X]\n",
    "        z=np.dot(dataX,self.theta)\n",
    "        y_pred=self.softmax(z)\n",
    "        print(y_pred)\n",
    "        return np.argmax(y_pred,axis=1)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpyarray.com - Matrix multiplication result:\n",
      " [[19 22]\n",
      " [43 50]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "A = np.array([[1, 2], [3, 4]])\n",
    "B = np.array([[5, 6], [7, 8]])\n",
    "\n",
    "C = np.dot(A, B)\n",
    "print(\"numpyarray.com - Matrix multiplication result:\\n\", C)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Labels: [0 2 1 2 0]\n",
      "One-Hot Encoded Labels:\n",
      " [[1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 假设 y_train 是你的原始标签数组\n",
    "y_train = np.array([0, 2, 1, 2, 0])\n",
    "\n",
    "# 使用 numpy 的 unique() 函数来获取所有唯一的标签\n",
    "unique_labels = np.unique(y_train)\n",
    "\n",
    "# 确定 one-hot 编码矩阵的列数\n",
    "num_classes = len(unique_labels)\n",
    "\n",
    "# 创建一个形状为 (len(y_train), num_classes) 的零矩阵\n",
    "y_train_one_hot = np.zeros((len(y_train), num_classes))\n",
    "\n",
    "# 创建一个从唯一标签到列索引的映射\n",
    "label_to_index = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "\n",
    "# 使用映射来设置正确的位置为 1\n",
    "y_train_one_hot[np.arange(len(y_train)), [label_to_index[label] for label in y_train]] = 1\n",
    "\n",
    "# 保存原始的标签值\n",
    "original_labels = y_train.copy()  # 使用 copy() 以确保不修改原始数组\n",
    "\n",
    "# 现在你可以使用 y_train_one_hot 进行训练或处理\n",
    "# 当你需要原始的标签时，可以引用 original_labels\n",
    "\n",
    "# 示例：打印原始的标签和 one-hot 编码的矩阵\n",
    "print(\"Original Labels:\", original_labels)\n",
    "print(\"One-Hot Encoded Labels:\\n\", y_train_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "        return np.exp(z) / np.sum(np.exp(z), axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.26894142, 0.73105858],\n",
       "       [0.11920292, 0.88079708]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=[[1,2],[2,4]]\n",
    "b=softmax(a)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3009932002481506"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred=np.array([0.1,0.2,0.3,0.4])\n",
    "y=np.array([0,0,1,0])\n",
    "samples_num=y.shape[0]\n",
    "\n",
    "loss=-np.sum(y*np.log(y_pred + 1e-9))/samples_num\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "import numpy as np  \n",
    "def one_hot_encode(y):\n",
    "    # 使用 numpy 的 unique() 函数来获取所有唯一的标签\n",
    "    unique_labels = np.unique(y)\n",
    "    # 确定 one-hot 编码矩阵的列数\n",
    "    num_classes = len(unique_labels)\n",
    "    # 创建一个形状为 (len(y_trai), num_classes) 的零矩阵\n",
    "    y_train_one_hot = np.zeros((len(y), num_classes))\n",
    "    # 创建一个从唯一标签到列索引的映射\n",
    "    label_to_index = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "    # 使用映射来设置正确的位置为 1\n",
    "    y_train_one_hot[np.arange(len(y)), [label_to_index[label] for label in y]] = 1\n",
    "    return y_train_one_hot\n",
    "\n",
    "def evaluate_classifier_multiple_times(classifier, X, y, learning_rate=0.01, num_iterations=1000,verbose=False,n_iterations=10):  \n",
    "    all_accuracies = []  \n",
    "    all_f1_scores = []  \n",
    "  \n",
    "    for iteration in range(n_iterations):  \n",
    "        # 设置十折交叉验证，每次使用不同的random_state  \n",
    "        kf = KFold(n_splits=10, shuffle=True, random_state=42 + iteration * 10)  # 使用迭代次数作为随机种子  \n",
    "        scores = []  \n",
    "        f1_scores_iter = []  \n",
    "  \n",
    "        # 遍历交叉验证的每一折  \n",
    "        for fold, (train_index, test_index) in enumerate(kf.split(X)):  \n",
    "            X_train, X_test = X.iloc[train_index], X.iloc[test_index]  \n",
    "            y_train, y_test = y.iloc[train_index], y.iloc[test_index]  \n",
    "  \n",
    "            # 将pandas读取的数据转化为list形式\n",
    "            X_train = X_train.values.tolist()  \n",
    "            y_train = y_train.values.tolist()  \n",
    "            # X_test = X_test.values.tolist()  \n",
    "            y_test = y_test.values.tolist()  \n",
    "           \n",
    "            X_train = np.array(X_train)\n",
    "            y_train = np.array(y_train)\n",
    "\n",
    "            y_train_one_hot=one_hot_encode(y_train)\n",
    "            # y_test_one_hot=one_hot_encode(y_test)\n",
    "            \n",
    "            \n",
    "            lr = classifier.LogisticRegression(X_train,y_train_one_hot, learning_rate, num_iterations,verbose=verbose)\n",
    "            # print('训练后参数',lr.theta)\n",
    "            y_pred=lr.predict(X_test)\n",
    "            # one_hot索引原坐标问题，本预测要求，数据集标签从1开始，而预测的标签从0开始且连续不间断，所以预测的标签需要+1\n",
    "            # Original Labels: [0 2 1 2 0]\n",
    "            # One-Hot Encoded Labels:\n",
    "            # [[1. 0. 0.]\n",
    "            # [0. 0. 1.]\n",
    "            # [0. 1. 0.]\n",
    "            # [0. 0. 1.]\n",
    "            # [1. 0. 0.]]\n",
    "\n",
    "            y_pred=y_pred+1\n",
    "            # y_pred_one_hot=one_hot_encode(y_pred)\n",
    "            # print('y_pred',y_pred,len(y_pred))\n",
    "            # print('y_test',y_test,len(y_test))\n",
    "            accuracy = accuracy_score(y_true=y_test, y_pred=y_pred)  \n",
    "            f1 = f1_score(y_test, y_pred, average='macro')  \n",
    "            scores.append(accuracy)  \n",
    "            f1_scores_iter.append(f1)  \n",
    "  \n",
    "        mean_accuracy = np.mean(scores)  \n",
    "        std_accuracy = np.std(scores)  \n",
    "        mean_f1 = np.mean(f1_scores_iter)  \n",
    "        print(f'第{iteration}次',mean_accuracy)  \n",
    "        all_accuracies.append(mean_accuracy)  \n",
    "        all_f1_scores.append(mean_f1)  \n",
    "  \n",
    "        # print(f\"Iteration {iteration + 1}: Mean Accuracy = {mean_accuracy:.4f}, Std Accuracy = {std_accuracy:.4f}, Mean F1 Score = {mean_f1:.4f}\")  \n",
    "\n",
    "    overall_mean_accuracy = np.mean(all_accuracies)  \n",
    "    overall_std_accuracy = np.std(all_accuracies)  \n",
    "    overall_mean_f1 = np.mean(all_f1_scores)  \n",
    "  \n",
    "    return overall_mean_accuracy, overall_std_accuracy, overall_mean_f1  \n",
    "  \n",
    "# # 示例调用  \n",
    "# # classifier_instance = YourClassifier()  # 替换为你的分类器实例  \n",
    "# # X = your_X_data  # 替换为你的特征数据  \n",
    "# # y = your_y_data  # 替换为你的标签数据  \n",
    "# # k = your_k_value  # 替换为你的k值  \n",
    "# # evaluate_classifier_multiple_times(classifier_instance, X, y, k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第0次 0.8607782898105478\n",
      "第1次 0.8687660010240655\n",
      "第2次 0.8592933947772657\n",
      "第3次 0.8686379928315413\n",
      "第4次 0.8655145929339477\n",
      "第5次 0.8609831029185868\n",
      "第6次 0.8671274961597542\n",
      "第7次 0.867178699436764\n",
      "第8次 0.8624935995903737\n",
      "第9次 0.8624423963133641\n",
      "data\\bal.xls \n",
      "  mean_accuracy: 0.864 std_accuracy: 0.003 f1: 0.599\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.discriminant_analysis import StandardScaler\n",
    "from package_py import LogisticRegression6\n",
    "\n",
    "    \n",
    "file_paths =[ \"data\\\\bal.xls\"]  # 实际文件路径\n",
    "# file_paths =[ \"data\\\\bal.xls\", \"data\\\\gla.xls\", \"data\\\\hay.xls\", \"data\\\\iri.xls\", \"data\\\\new.xls\", \"data\\\\win.xls\", \"data\\\\zoo.xls\"]  # 实际文件路径\n",
    "# mean_accuracys=[]\n",
    "for i in range(len(file_paths)):\n",
    "    file_path=file_paths[i]\n",
    "\n",
    "    data = pd.read_excel(file_path, header=None)  \n",
    "    # 将数据分为特征和标签  \n",
    "    X = data.iloc[:, :-1]  # 前n列是特征  \n",
    "    y = data.iloc[:, -1]   # 最后一列是分类标签  \n",
    "    \n",
    "    # 数据标准hua\n",
    "    scaler = StandardScaler()  \n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    X_scaled_df = pd.DataFrame(X_scaled, columns=X.columns)\n",
    "    \n",
    "    mean_accuracy,std_accuracy,f1=evaluate_classifier_multiple_times(LogisticRegression6,X_scaled_df,y,n_iterations=10,learning_rate=0.1, num_iterations=1000,verbose=False)\n",
    "    # mean_accuracys.append(mean_accuracy)\n",
    "\n",
    "    # 使用 f-string 格式化输出  \n",
    "    print(f'{file_path} \\n  mean_accuracy: {mean_accuracy:.3f} std_accuracy: {std_accuracy:.3f} f1: {f1:.3f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "调库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据集data\\zoo.xls第1次十折准确度: 0.9400000000000001\n",
      "数据集data\\zoo.xls第2次十折准确度: 0.93\n",
      "数据集data\\zoo.xls第3次十折准确度: 0.93\n",
      "数据集data\\zoo.xls第4次十折准确度: 0.9400000000000001\n",
      "数据集data\\zoo.xls第5次十折准确度: 0.9227272727272726\n",
      "数据集data\\zoo.xls第6次十折准确度: 0.9109090909090909\n",
      "数据集data\\zoo.xls第7次十折准确度: 0.941818181818182\n",
      "数据集data\\zoo.xls第8次十折准确度: 0.9100000000000001\n",
      "数据集data\\zoo.xls第9次十折准确度: 0.93\n",
      "数据集data\\zoo.xls第10次十折准确度: 0.9200000000000002\n",
      "数据集data\\zoo.xls平均准确度: 0.9275454545454546\n",
      "数据集data\\zoo.xls平均F1分数: 0.8421820910163478\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.linear_model import LogisticRegression  # 使用逻辑斯蒂回归\n",
    "from sklearn.model_selection import KFold, cross_val_score, cross_val_predict\n",
    "import numpy as np\n",
    "# 文件路径列表\n",
    "file_paths = [r\"data\\zoo.xls\"]\n",
    "# # 文件路径列表\n",
    "# file_paths = [\n",
    "#     r\"data\\bal.xls\", r\"data\\gla.xls\", r\"data\\hay.xls\",\n",
    "#     r\"data\\iri.xls\", r\"data\\new_avoid_negtive.xls\", r\"data\\win.xls\", r\"data\\zoo.xls\"\n",
    "# ]\n",
    "# 初始化结果字典\n",
    "results = {}\n",
    "# 对每个数据集进行十次十折交叉验证\n",
    "for file_path in file_paths:\n",
    "    # 读取Excel文件\n",
    "    df = pd.read_excel(file_path, header=None)\n",
    "    # 分离特征和标签\n",
    "    X = df.iloc[:, :-1]  # 所有行，除了最后一列的所有列（特征）\n",
    "    y = df.iloc[:, -1]   # 所有行，最后一列（标签）\n",
    "    # 确保标签是整数类型（对于sklearn的分类器通常是必要的）\n",
    "    y = y.astype(int)\n",
    "    # 创建逻辑斯蒂回归分类器，如果类别不平衡，可以考虑使用class_weight='balanced'\n",
    "    lr = LogisticRegression(max_iter=1000, random_state=42, solver='liblinear')  # max_iter增加以避免收敛警告，solver选择适合小数据集的'liblinear'\n",
    "    # 初始化用于存储每次交叉验证结果的列表\n",
    "    accuracies = []\n",
    "    f1_scores_list = []\n",
    "    # 进行十次十折交叉验证\n",
    "    for i in range(10):\n",
    "        kf = KFold(n_splits=10, shuffle=True, random_state=42 + i * 10)\n",
    "        scores = cross_val_score(lr, X, y, cv=kf, scoring='accuracy')\n",
    "        accuracies.append(scores.mean())\n",
    "        \n",
    "        # 使用cross_val_predict获取所有折叠的预测\n",
    "        y_preds = cross_val_predict(lr, X, y, cv=kf)\n",
    "        # 计算F1分数（macro平均）\n",
    "        f1_scores = f1_score(y, y_preds, average='macro')\n",
    "        f1_scores_list.append(f1_scores)\n",
    "        print(f\"数据集{file_path}第{i+1}次十折准确度: {scores.mean()}\")\n",
    "    \n",
    "    # 计算十次交叉验证的平均准确度和F1分数\n",
    "    mean_accuracy = np.mean(accuracies)\n",
    "    mean_f1 = np.mean(f1_scores_list)\n",
    "    print(f\"数据集{file_path}平均准确度: {mean_accuracy}\")\n",
    "    print(f\"数据集{file_path}平均F1分数: {mean_f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Loss: 1.098612\n",
      "Iteration 100, Loss: 0.461157\n",
      "Iteration 200, Loss: 0.357088\n",
      "Iteration 300, Loss: 0.307805\n",
      "Iteration 400, Loss: 0.277485\n",
      "Iteration 500, Loss: 0.256388\n",
      "Iteration 600, Loss: 0.240606\n",
      "Iteration 700, Loss: 0.228216\n",
      "Iteration 800, Loss: 0.218147\n",
      "Iteration 900, Loss: 0.209748\n",
      "Iteration 1000, Loss: 0.202596\n",
      "Iteration 1100, Loss: 0.196404\n",
      "Iteration 1200, Loss: 0.190969\n",
      "Iteration 1300, Loss: 0.186144\n",
      "Iteration 1400, Loss: 0.181818\n",
      "Iteration 1500, Loss: 0.177907\n",
      "Iteration 1600, Loss: 0.174344\n",
      "Iteration 1700, Loss: 0.171079\n",
      "Iteration 1800, Loss: 0.168069\n",
      "Iteration 1900, Loss: 0.165280\n",
      "Iteration 2000, Loss: 0.162684\n",
      "Iteration 2100, Loss: 0.160259\n",
      "Iteration 2200, Loss: 0.157984\n",
      "Iteration 2300, Loss: 0.155844\n",
      "Iteration 2400, Loss: 0.153825\n",
      "Iteration 2500, Loss: 0.151914\n",
      "Iteration 2600, Loss: 0.150101\n",
      "Iteration 2700, Loss: 0.148377\n",
      "Iteration 2800, Loss: 0.146735\n",
      "Iteration 2900, Loss: 0.145167\n",
      "[[9.89818100e-01 1.19129530e-05 1.01699872e-02]\n",
      " [2.49025436e-01 3.15834059e-02 7.19391158e-01]\n",
      " [5.67708383e-01 2.58192145e-01 1.74099472e-01]\n",
      " [7.48927543e-01 1.40436725e-05 2.51058413e-01]\n",
      " [5.96302837e-04 1.02205580e-07 9.99403595e-01]\n",
      " [1.35442004e-02 5.72517332e-04 9.85883282e-01]\n",
      " [5.25315592e-01 2.36918118e-02 4.50992597e-01]\n",
      " [9.52162405e-01 5.23564298e-06 4.78323598e-02]\n",
      " [9.99855363e-01 1.22554729e-06 1.43411013e-04]\n",
      " [3.06548823e-03 2.39809099e-05 9.96910531e-01]]\n",
      "Predictions: [0 2 0 0 2 2 0 0 0 2]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 创建测试数据集（使用随机数据）\n",
    "np.random.seed(42)  # 为了结果可重复\n",
    "X = np.random.randn(100, 3)  # 100个样本，每个样本3个特征\n",
    "true_theta = np.array([[1.0, -2.0, 1.0], [-1.0, 1.5, -0.5], [0.5, -1.0, 2.0], [1.0, -2.0, 1.0]])  # 假设的真实参数\n",
    "y_true_indices = np.argmax(np.dot(np.c_[np.ones((100, 1)), X], true_theta), axis=1)  # 计算每个样本的类别索引\n",
    "y = np.eye(3)[y_true_indices]  # 将类别索引转换为one-hot编码\n",
    "# print(\"y\",y)\n",
    "\n",
    "# 创建逻辑回归模型实例\n",
    "model = LogisticRegression(X, y, learning_rate=0.08, num_iterations=3000, solver='gd', verbose=True)\n",
    "\n",
    "# 预测\n",
    "X_test = np.random.randn(10, 3)  # 10个测试样本，每个样本3个特征\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# 打印预测结果\n",
    "print(\"Predictions:\", predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.06578852 -2.21535124  1.14956272]\n",
      " [-1.76676584  1.97636132 -0.20959549]\n",
      " [-1.66673075 -2.42355979  4.09029054]\n",
      " [ 1.38226106 -3.22177144  1.83951039]]\n"
     ]
    }
   ],
   "source": [
    "print(model.theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21,)\n",
      "(21, 1)\n",
      "(21,)\n",
      "(21,)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "non-broadcastable output operand with shape (21,) doesn't match the broadcast shape (21,21)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_10876\\348074824.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    118\u001b[0m     \u001b[1;31m# 创建逻辑斯蒂回归模型并训练\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.01\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_iterations\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msolver\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'sgd'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mC\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 120\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    121\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m     \u001b[1;31m# 评估模型\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_10876\\348074824.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m     32\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gradient_descent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_b\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msolver\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'sgd'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stochastic_gradient_descent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_b\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Unsupported solver. Use 'gd' for gradient descent or 'sgd' for stochastic gradient descent.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_10876\\348074824.py\u001b[0m in \u001b[0;36m_stochastic_gradient_descent\u001b[1;34m(self, X_b, y)\u001b[0m\n\u001b[0;32m     71\u001b[0m                 \u001b[1;31m# 但在实践中，学习率对于sgd通常是固定的，并且我们不会在这里除以m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m                 \u001b[1;31m# 下面的更新是简化的，没有除以m，但加入了正则化项和C的倒数\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweights\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearning_rate\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mgradient\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mregularization\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m             \u001b[1;31m# 注意：这里没有检查收敛条件，因为SGD通常依赖于足够多的迭代次数来收敛\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: non-broadcastable output operand with shape (21,) doesn't match the broadcast shape (21,21)"
     ]
    }
   ],
   "source": [
    "import numpy as np  \n",
    "from sklearn.model_selection import train_test_split  \n",
    "from sklearn.preprocessing import StandardScaler  \n",
    "from sklearn.metrics import accuracy_score  \n",
    "  \n",
    "class LogisticRegression:  \n",
    "    def __init__(self, learning_rate=0.01, num_iterations=1000, solver='gd', tol=1e-4, C=1.0, verbose=False):  \n",
    "        self.learning_rate = learning_rate  \n",
    "        self.num_iterations = num_iterations  \n",
    "        self.solver = solver  # 'gd' for gradient descent, 'sgd' for stochastic gradient descent  \n",
    "        self.tol = tol  # tolerance for stopping criteria  \n",
    "        self.C = C  # regularization strength (inverse of regularization strength if solver is 'sgd')  \n",
    "        self.verbose = verbose  \n",
    "        self.weights = None  \n",
    "        self.bias = None  \n",
    "  \n",
    "    def fit(self, X, y):  \n",
    "        # 数据预处理  \n",
    "        self.scaler = StandardScaler()  \n",
    "        X_scaled = self.scaler.fit_transform(X)  \n",
    "        self.m, self.n = X_scaled.shape  \n",
    "  \n",
    "        # 添加偏置项  \n",
    "        X_b = np.c_[np.ones((self.m, 1)), X_scaled]  \n",
    "  \n",
    "        # 初始化参数  \n",
    "        self.weights = np.zeros(self.n + 1)  \n",
    "        self.bias = self.weights[-1]  # 偏置项是权重向量中的最后一个元素  \n",
    "  \n",
    "        # 梯度下降或随机梯度下降  \n",
    "        if self.solver == 'gd':  \n",
    "            self._gradient_descent(X_b, y)  \n",
    "        elif self.solver == 'sgd':  \n",
    "            self._stochastic_gradient_descent(X_b, y)  \n",
    "        else:  \n",
    "            raise ValueError(\"Unsupported solver. Use 'gd' for gradient descent or 'sgd' for stochastic gradient descent.\")  \n",
    "  \n",
    "    def _gradient_descent(self, X_b, y):  \n",
    "        for _ in range(self.num_iterations):  \n",
    "            y_predicted_prob = self._sigmoid(np.dot(X_b, self.weights))  \n",
    "            gradient = (1 / self.m) * np.dot(X_b.T, (y_predicted_prob - y))  \n",
    "            regularization = (self.C / self.m) * self.weights  \n",
    "            regularization[0] = 0  # 不对偏置项进行正则化  \n",
    "  \n",
    "            # 更新权重  \n",
    "            self.weights -= self.learning_rate * (gradient + regularization)  \n",
    "  \n",
    "            # 检查收敛条件（基于梯度的L2范数）  \n",
    "            if np.linalg.norm(gradient + regularization, ord=2) < self.tol:  \n",
    "                break  \n",
    "  \n",
    "            if self.verbose and _ % 100 == 0:  \n",
    "                loss = self._compute_loss(y, y_predicted_prob)  \n",
    "                print(f\"Iteration {_}: loss = {loss}\")  \n",
    "  \n",
    "    def _stochastic_gradient_descent(self, X_b, y):  \n",
    "        for _ in range(self.num_iterations):  \n",
    "            for i in range(self.m):  \n",
    "                xi_b = X_b[i:i+1, :]  \n",
    "                yi = y[i:i+1]  \n",
    "                y_predicted_prob = self._sigmoid(np.dot(xi_b, self.weights))  \n",
    "                gradient = xi_b.T * (y_predicted_prob - yi)  \n",
    "                regularization = (self.m / self.C) * self.weights  \n",
    "                \n",
    "                regularization[0] = 0  # 不对偏置项进行正则化  \n",
    "                print(gradient.shape)\n",
    "                print(regularization.shape)\n",
    "                \n",
    "                print(self.weights.shape)\n",
    "                # 更新权重（注意这里的学习率需要除以m在sgd中通常不这样做，但为了与L2正则化项匹配，我们这样做）  \n",
    "                # 但在实践中，学习率对于sgd通常是固定的，并且我们不会在这里除以m  \n",
    "                # 下面的更新是简化的，没有除以m，但加入了正则化项和C的倒数  \n",
    "                self.weights -= self.learning_rate * (gradient + regularization / self.m)  \n",
    "  \n",
    "            # 注意：这里没有检查收敛条件，因为SGD通常依赖于足够多的迭代次数来收敛  \n",
    "            # 可以在实际应用中添加额外的停止条件，如验证集上的性能不再提升  \n",
    "  \n",
    "            if self.verbose and _ % 100 == 0:  \n",
    "                # 由于SGD是随机的，我们不能简单地在每次迭代时计算整个数据集的损失  \n",
    "                # 但为了演示目的，我们可以计算并打印一个批次或整个数据集的损失（不推荐在每次迭代时都这样做）  \n",
    "                # 下面的代码计算了整个数据集的损失，这在实际SGD中是不切实际的  \n",
    "                y_pred_prob_all = self._sigmoid(np.dot(X_b, self.weights))  \n",
    "                loss = self._compute_loss(y, y_pred_prob_all)  \n",
    "                print(f\"Iteration {_}: loss (for demonstration purposes) = {loss}\")  \n",
    "  \n",
    "    def predict_prob(self, X):  \n",
    "        # 数据预处理（使用训练时拟合的scaler）  \n",
    "        X_scaled = self.scaler.transform(X)  \n",
    "        X_b = np.c_[np.ones((X_scaled.shape[0], 1)), X_scaled]  \n",
    "        return self._sigmoid(np.dot(X_b, self.weights))  \n",
    "  \n",
    "    def predict(self, X, threshold=0.5):  \n",
    "        return self.predict_prob(X) >= threshold  \n",
    "  \n",
    "    def _sigmoid(self, z):  \n",
    "        return 1 / (1 + np.exp(-z))  \n",
    "  \n",
    "    def _compute_loss(self, y_true, y_pred):  \n",
    "        m = y_true.shape[0]  \n",
    "        epsilon = 1e-15  \n",
    "        y_pred = np.clip(y_pred, epsilon, 1 - epsilon)  \n",
    "        return - (1 / m) * np.sum(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))  \n",
    "  \n",
    "    def score(self, X, y):  \n",
    "        y_pred = self.predict(X)  \n",
    "        return accuracy_score(y, y_pred)  \n",
    "  \n",
    "# 示例用法  \n",
    "if __name__ == \"__main__\":  \n",
    "    # 假设我们有一些数据（这里使用一个简单的二分类数据集作为示例）  \n",
    "    from sklearn.datasets import make_classification  \n",
    "    X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)  \n",
    "    y = y.reshape(-1, 1)  # 确保y是列向量  \n",
    "  \n",
    "    # 划分训练集和测试集  \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)  \n",
    "  \n",
    "    # 创建逻辑斯蒂回归模型并训练  \n",
    "    model = LogisticRegression(learning_rate=0.01, num_iterations=5000, solver='sgd', C=1.0, verbose=True)  \n",
    "    model.fit(X_train, y_train)  \n",
    "  \n",
    "    # 评估模型  \n",
    "    accuracy = model.score(X_test, y_test)  \n",
    "    print(f\"Test set accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TF",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
